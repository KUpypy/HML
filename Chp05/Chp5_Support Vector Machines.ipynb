{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chp5.  Support Vector Machines\n",
    "\n",
    "- __발표일 : 2017. 12. 2(토)__\n",
    "- __발표자 : 이우진, 정지원__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM은 일반적으로 복잡하지만 중소 규모 사이즈의 데이터셋을 분류하는데 알맞다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](figures/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선 밖에 더 많은 인스턴스를 두는게 목적이 아니라(왼쪽) 선 위에 있는 인스턴스들에 의해 결정된다. 오른쪽 그림의 빨간 원으로 그려진 것들을 support vector라고 부른다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    SVM은 feature scaling에 민감하다. 밑의 왼쪽이 안한것, 오른쪽이 한것. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](figures/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 인스턴스가 street 오른쪽에 있으라고 한다면 이는 _hard margin classification_이다. 하지만 이는 인스턴트들이 선형 분리 가능할때만 작동하며, outlier에 굉장히 민감하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](figures/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 피하려면 좀더 유연한 모델이 필요하다. 따라서 _margin violations_(street의 중간이나 다른 side에 있는것도 허용)을 제한하면서 길을 가장 크게 만드는 밸런스를 찾는것이 목적이다. 이를 _soft margin classification_이라 부른다.\n",
    "\n",
    "Scikit-Learn의 SVM클래스에서 이 밸런스를 `C` hyperparameter를 조정함으로써 조정할수 있다. 값이 작을수록 wider street(more margin violations) 를 의미한다. 밑 그림 오른쪽이 더 나은데, margin violation이 많아도 generalize를 잘 했기 때문이다.\n",
    "\n",
    "    SVM모델이 overfitting하면, C 값 내리자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](figures/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "밑 코드에서 hinge loss function을 사용했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=(('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n",
    "svm_clf = Pipeline(((\"scaler\", StandardScaler()),(\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),))\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    SVM 분류기는 Logistic Regression과 다르게 확률값을 리턴하지 않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 방법은 `SGDClassifier(loss=\"hindge\", alpha=1/(m*C))` 클래스를 이용해 SVM을 훈련시키는 것이다. 이는 `LinearSVC`클래스보다는 느리지만, 큰 데이터셋을 처리하거나 online classification 을 할때 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its mean. This is automatic if you scale the data using the StandardScaler. Moreover, make sure you set the loss hyperparameter to \"hinge\", as it is not the default value. Finally, for better performance you should set the dual hyperparameter to False, unless there are more features than training instances (we will discuss duality later in the chapter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왼쪽 그림을 보면, 하나의 feature만 있는데도 선형 분리가 되지 않는다. 이럴때 $x_2 =(x_1)^2$를 만들어 주면, 선형 분리가 가능해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](figures/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=(('poly_features', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_svm_clf = Pipeline((\n",
    "            (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "        ))\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](figures/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Polynomial Kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "polynomial feature를 추가하는것은 그리 어렵지 않지만, 너무 낮은 polynomial degree라면 복잡한 데이터를 잘 설명하지 못할 것이다. 반면 너무 높은 polynomial degree를 가진다면 모델을 너무 느리게 만들 것이다. \n",
    "\n",
    "운좋게도, SVM을 할때 _kernel trick_이라는 혁신적 수학 테크닉을 사용할수 있다. 이는 우리가 많은 polynomial feature를 추가해도 실제로 그들을 더하지 않으면서 같은 결과를 얻을 수 있게 해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=(('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline((\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))    \n",
    "        ))\n",
    "poly_kernel_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "위 코드는 3rd degree의 polynomial kernel를 사용한 SVM 분류기를 훈련시킨다. 오른쪽 사진은 다른 10th degree의 다항 커널이다. 모델이 오버피팅하면 degree를 낮추고, underfitting하면 올리게 될 것이다. `coef0` 하이퍼파라메터가 곧 모델이 얼마나 high degree polynomial에 영향 받을 것인지 조절한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Image](figures/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A common approach to find the right hyperparameter values is to use grid search (see Chapter 2). It is often faster to first do a very coarse grid search, then a finer grid search around the best values found. Having a good sense of what each hyperparameter actually does can also help you search in the right part of the hyperparameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Similarity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비선형 문제를 다루는 다른 기법은 _similarity function_을 이용해 각 인스턴스가 특정 _landmark_와 얼마나 가까이 있는지 계산된 feature를 추가하는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](figures/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](figures/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](figures/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 landmark를 어떻게 세울것인가? 가장 쉬운 방법은 모든 데이터셋에 랜드마크를 세우는 것이다. 이는 차원을 많이 만들 것이고 곧 이는 훈련셋이 선형적으로 분리 가능하도록 만들 가능성을 높일 것이다. 단점은 m instance and n features 가 곧 m instances and m feature로 바뀐다는 것이다(=원래 feature들을 없앤다). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아까와 같이 하는것은 computation cost가 너무 높을수 있다(큰 데이터라면). 하지만 다시한번 커널 트릭은 SVM magic을 부린다: 이는 많은 similarity feature를 더한것과 비슷한 결과를 낸다(실제로 더 더하지 않고). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=(('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=5, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline((\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "))\n",
    "rbf_kernel_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](figures/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "모델은 왼쪽 아래 설명되어 있다. `gamma`를 늘리면 종모양 커브(RBF)를 좁게 만든다. 따라서 각 인스턴스의 영향 정도가 작아진다: 결정 바운더리사 더 불규칙해지게 된다. 역으로 `gamma`를 작게 만들면 더 넓은 종모양을 만들게 되어서, 인스턴스가 larger range of influence를 가지게 된다. 따라서 `gamma`는 마치 정규화 하이퍼파라메터처럼 기능한다. 오버피팅시에는 줄이고, 아니면 그 반대로 하면 된다(C 하이퍼파라메터처럼)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    With so many kernels to choose from, how can you decide which one to use? As a rule of thumb, you should always try the linear kernel first (remember that LinearSVC is much faster than SVC(ker nel=\"linear\")), especially if the training set is very large or if it has plenty of features. If the training set is not too large, you should try the Gaussian RBF kernel as well; it works well in most cases. Then if you have spare time and computing power, you can also experiment with a few other kernels using cross-validation and grid search, especially if there are kernels specialized for your training set’s data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LinearSVC`클래스는 _liblinear_라이브러리(선형 SVM을 위한 최적화 알고리즘 implement)에 있다. 이는 커널 트릭은 제공 안하지만, 거의 선형적으로 scale한다. -> $O(m*n) $\n",
    "\n",
    "high precision을 원한다면 더 오래 걸린다. 이는 `tol`이라는 하이퍼파라메터(tolerance)로 정해진다. 디폴트값이 거의 대부분 상황에서 괜찮다.\n",
    "\n",
    "`SVC`클래스는 kernel trick을 지원하는 `libsvm`라이브러리에 기반한다. 따라서 보통 $O(m^2 \\times n) 과 O(m^3 \\times n) 사이의 복잡도를 가진다. 불행하게도, 이는 데이터가 많아지면 엄청나게 느리다는 것을 뜻한다. 이는 작지만 복잡한 데이터에 알맞다. 하지만, 피쳐의 갯수에 따라 스케일을 잘 한다 (특히나 sparse feature에 대해). 이런 케이스에서는 잘 하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](figures/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    "\n",
    "언급했듯이, SVM 알고리즘은 다소 변하기가 쉽다. linear/nonlinear classfication 뿐만 아니라, linear/nonlinear regression도 지원한다.\n",
    "\n",
    "objective를 reverse하는 트릭을 사용한다. 마진 위반을 제한하면서 두 클래스 사이의 가능한 가장 큰 거리를 맞추는 방법 대신에, SVM regression은 마진위반을 제한하면서 최대한 많은 instances를 영역에 들어오도록 맞춰진다. 거리의 width는 아래 그림과 같이 hypterparameter $\\epsilon$에 의해 제어된다.\n",
    "\n",
    "![Image](figures/13.png)\n",
    "\n",
    "더 많은 instances를 margin 안쪽에 추가한다고 해서 모델의 예측에 영향을 주지는 않는다. 즉 모델은 $\\epsilon$을 신경쓰지 않는다.\n",
    "\n",
    "scikit-learn의 Linear SVM을 사용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비선형 회귀작업을 수행하기 위해, kernelized SVM model을 사용할 수 있다. 예를 들어, 아래 그림은 2차 다항식 커널을 사용하여 무작위 2차 training set에 적용한 것이다. 왼쪽 그림은 약간의 Regularization을 했으며, 오른쪽이 더 정규화를 많이 한 것이다.\n",
    "\n",
    "![Image](figures/14.png)\n",
    "\n",
    "아래 코드는 왼쪽 그림의 SVR에 대한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='auto',\n",
       "  kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the Hood\n",
    "\n",
    "이 섹션에서는 선형 SVM 분류기를 통해 어떻게 SVM이 예측을 하고, 학습 알고리즘이 작동하는지에 대해 알아본다. 머신러닝이 처음이라면 여기서부터는 건너뛰고 챕터를 마무리하고, 나중에 더 이해하고 싶을 때 돌아와도 좋다.\n",
    "\n",
    "우선, notation에 대해 다시 얘기한다. Chp4에서 모든 모델 파라미터를 bias term인 $\\theta_0$를 포함하여 하나의 벡터 $\\theta$에 넣었으며, $x_{0} = 1$로 정의했엇다. 이 챕터에서는 다른 방식을 사용하는데, 보통 SVM을 다룰 때는 이처럼 한다.(이게 더 편리하다.) bias term을 $b$라고 하며, weights vector를 __$w$__로 정의한다. 따라서 $x_{0}$과 같은 term은 더이상 쓰지 않아도 된다.\n",
    "\n",
    "### Decision funcion and Predictions\n",
    "\n",
    "Linear SVM classifier 모델은 새로운 instance x를 단순히 decision function을 통해 예측한다. $w^{T} \\cdot x + b = w_{1}x_{1} + ... + w_{n}x_{n}+b$ . 결과가 양수인지 음수인지에 따라 클레스를 나누게 된다. 아래 그림과 같다.\n",
    "\n",
    "![Image](figures/15.png)\n",
    "\n",
    "아래 그림의 Decisionn function은 위 Figure 5-4의 우측 그림에 대응되는 것이다. 데이터가 2개의 features를 갖기 때문에 2차원 평면이 생긴다. Decision boundary는 decision function이 0인 곳에서 생긴다. 이것은 두 평면과 교차하는 직선과 같다.\n",
    "\n",
    "![Image](figures/16.png)\n",
    "\n",
    "점선은 decision function이 1이거나 -1인(support vector) 점들을 나타낸다. 결국 SVM classifier를 학습한다는 것은 margin 위반을 피하거나(hard margin) 그 한계를 제한하면서(soft margin) margin을 가장 크게 할 수 있는 $w$와 $b$를 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Objective\n",
    "\n",
    "Decision function의 기울기를 고려해보자. 이것은 weights vector의 nrom과 같다.($∥w∥$) 이 기울기를 2로 나눈 경우, decision function이 ±1인 점은 decision boundary에서 두 배 멀리 떨어진다. 다시 말하면, 기울기를 2로 나누는 것은 margin에 2를 곱하는 것과 같다. 아래 그림을 통해 보는 것이 좋을 것이다. weight vector가 작아지면, margin이 커진다.\n",
    "\n",
    "![Image](figures/17.png)\n",
    "\n",
    "따라서, 우리는 큰 margin을 얻기 위해 ∥w∥를 minimize 하기를 원한다. 그러나, 마진 위반을 피하길 원하기 때문에(hard margin) decision function이 모든 positive instances에 대해서는 1보다 크고, negative instances에 대해서는 -1보다 작도록 해야한다. 우리가 negative instances를 t(i) = -1로 정의한고, positive instacnes를 t(i)=1로 지정한다면, 제약식을 다음과 같이 표현할 수 있다.\n",
    "\n",
    "$ t(i)(w^T \\cdot x(i) + b) ≥ 1$ for all instances.\n",
    "\n",
    "따라서 다음과 같은 제약식으로 표현할 수 있다.\n",
    "\n",
    "![Image](figures/18.png)\n",
    "\n",
    "soft margin objective를 구하기 위해선, 각 instance마다 slack variable인 ζ(i) ≥ 0를 도입해야 한다. ζ(i)는 i번째 instance가 얼마나 margin 위반을 허용할지에 대한 수치이다. 따라서 두개의 목표가 충돌한다. slack variable을 가능한 작게 가져가면서, 목적함수인 $\\frac{1}{2}w^{T}\\cdot w$을 최소화 해야한다. C라는 parameter가 이 trade-off 관계의 목적함수를 조절해준다.\n",
    "\n",
    "![Image](figures/19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Programming\n",
    "\n",
    "하드 마진과 소프트 마진 문제는 모두 선형 제약식을 가진 convex quadratic 최적화 문제이다. 이러한 문제는 QP(Quadratic Programming) 문제로 알려져 있다. 많은 solvers들이 이러한 QP 문제를 이 책의 범위를 벗어나는 다양한 방법을 통해 해결할 수 있으며 일반적인 방법은 아래와 같다.\n",
    "\n",
    "![Image](figures/20.png)\n",
    "\n",
    "\n",
    "식 $A \\cdot p ≤ b$ 는 실제 $n_c$개의 constraints를 의미한다. $p^T \\cdot a^{(i)} ≤ b^{(i)}$ for $i = 1, 2, ⋯, n_c$ where $a^{(i)}$ is the vector containing the elements of the $i^{th}$ row of A and $b^{(i)}$ is the $i^{th}$ element of b.\n",
    "\n",
    "다음과 같은 방법으로 QP parameter를 설정하면, 하드 마진 linear SVM classifier의 Objective를 얻는 과정을 쉽게 확인할 수 있다.\n",
    "\n",
    "- $n_{p} = n + 1$, where n is the number of features (the +1 is for the bias term).\n",
    "- $n_{c} = m$, where m is the number of training instances.\n",
    "- H is the $n_{p} × n_{p}$ identity matrix, except with a zero in the top-left cell (to ignore\n",
    "the bias term).\n",
    "- f = 0, an $n_{p}$-dimensional vector full of 0s.\n",
    "- b = 1, an $n_{c}$-dimensional vector full of 1s.\n",
    "- $a^{(i)} = –t^{(i)}\\dot x^{(i)}$, where  $\\dot x^{(i)}$ is equal to $x^{(i)}$ with an extra bias feature $\\dot x_{0} = 1.$\n",
    "\n",
    "하드마진 linear SVM classifier를 학습하는 한 가지 방법은, off-the-shelf QP solver를 이전의 parameter를 전달하여 사용하는 것이다. 결과 벡터인 p는 bias term $b=p_{0}$하고 feature weights $w_{i} = p_{i}$ for $i = 1, 2, ⋯, m$를 포함한다. 유사하게, QP solver를 이용하여 소프트 마진 문제를 해결할 수 있다. 그러나 kernel trick을 이용한다면, 이 문제를 다른 제약식을 갖는 최적화 문제로 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dual Problem\n",
    "\n",
    "어떤 Problem이 주어지면, 다른 문제이지만 밀접하게 관련된 problem을 표현할 수 있다. 이 문제를 Dual Problem이라고 한다. Dual 문제에 대한 해법은 일반적으로 primal 문제의 해법에 대한 경계를 낮추지만, 어떤 조건에서는 특별 문제와 동일한 해를 가질 수도 있다. 운좋게도 SVM 문제는 이러한 조건을 충족 시키므로, 원래 문제를 Dual의 문제로 해결할 수 있다. 두 가지 모두 동일한 솔루션을 갖게 된다.(Primal 문제에서 Dual 문제를 유도하는 방법을 알고 싶다면 부록 C 참조).\n",
    "\n",
    "![Image](figures/21.png)\n",
    "\n",
    "이 방정식을 최소화하는 벡터 $\\alpha$를 찾으면, 아래 식을 사용하여 원래 문제를 최소화하고 계산할 수 있다.\n",
    "\n",
    "![Image](figures/22.png)\n",
    "\n",
    "Dual problem은 training instances가 features의 수보다 작을 때 원래 문제보다 더 빠르게 풀 수 있다. 더 중요한 것은, primal은 하지 못하는 kernel trick을 사용할 수 있다. 그렇다면 커널 트릭은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized SVM\n",
    "\n",
    "2차원 다항식 변환을 2차원 학습 셋에 적용한 다음, 변형된 학습 셋에서 선형 SVM 분류기를 학습한다고 가정한다. 아래 식은 2차 다항식 매핑 함수 φ를 보여준다.\n",
    "\n",
    "![Image](figures/23.png)\n",
    "\n",
    "변환된 벡터는 2차원이 아닌 3차원이다. 이제 2차 다항식 매핑을 적용하고 변환된 벡터의 내적을 계산하면, 2차원 벡터 2개에 어떤 일이 일어나는지 보자.\n",
    "\n",
    "![Image](figures/24.png)\n",
    "\n",
    "어떠한가? 변환된 벡터의 내적은 원래 벡터의 dot product의 square와 같다.\n",
    "\n",
    "- #### $\\Phi(a)^{T} \\cdot \\Phi(b) = (a^{T} \\cdot b)^{2}$\n",
    "\n",
    "여기에 중요한 insight가 있다. 모든 학습 instances에 φ를 적용한다면, Dual 문제는 dot product를 포함할 것이다. 그러나, $\\Phi$가 위에서 얘기했던 polynomial transformation이라면, 이를 그냥 단순한 $(x^{(i)^{T}} \\cdot x^{(j)})^{2}$로 구할 수 있다.\n",
    "\n",
    "따라서, 실제로 transform을 하지 않아도 되며, 단순히 dot product에 square를 입혀주기만 하면 된다. 결과는 완전 같을 것이지만, 실제 이러한 트릭은 아주 많은 컴퓨터 효율을 가져다 준다. 이것이 커널 트릭의 본질이다.\n",
    "\n",
    "![Image](figures/25.png)\n",
    "\n",
    "풀어야 할 문제가 하나 남았다. 식 5-7는 dual solution을 primal soultion으로 바꾸는 방법을 보여줬다. 그러나 커널트릭을 사용하면, 우리는 $φ(x(i))$가 포함된 식을 얻게 된다. 사실, $\\widehat{w}$는 φ(x(i))와 같은 차원수를 가져야 한다. 아주 크거나 거의 무한대의 크기를 갖기 때문에 실제 계산할 수가 없다. 그러나, $\\widehat{w}$를 모르고서 어떻게 예측을 할 수 있을까? 좋은 소식은 식 5-7의 w에 대한 식을 새로운 instance $x^{n}$에 대한 decision function에 적용할 수 있다는 것이다. 그러면 입력 벡터 사이에만 dot product가 있는 식을 얻을 수 있다. 이렇게 하면 다시 커널 트릭을 사용할 수 있다.\n",
    "\n",
    "![Image](figures/26.png)\n",
    "\n",
    "Support vectors에 대해서만 α(i) ≠ 0 이기 때문에, 예측을 하는 것은 모든 training instances가 아닌, support vector와 새로운 입력 벡터와의 내적을 하는 것을 의미한다. 물론, 같은 트릭을 이용하여 bias b를 계산해야 한다.\n",
    "\n",
    "![Image](figures/27.png)\n",
    "\n",
    "머리가 아파오면, 정상이다. 커널 트릭의 불행한 부작용이다.\n",
    "\n",
    "#### Mercer’s Theorem\n",
    "\n",
    "> Mercer’s theorem에 의하면, 함수 K(a, b)가(K는 연속적이고 대칭인 인수이므로 K(a, b)=K(b, a)) Mercer의 조건이라고 하는 몇 가지 수학 조건을 respect한다면, $K(a,b)=\\Phi(a)^{T} \\cdot \\Phi(b)$가 되도록 다른 공간에 a와 b를 매핑하는 함수 $\\Phi$가 존재한다. 따라서 $\\Phi$가 무엇인지 모를지라도 $\\Phi$가 존재한다는 것을 알기 때문에 K를 커널로 사용할 수 있다. Gaussian RBF 커널의 경우, $\\Phi$가 실제로 각 학습 인스턴스를 무한 차원 공간에 매핑한다는 것을 알 수 있으므로 매핑을 실제로 수행 할 필요가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online SVMs\n",
    "\n",
    "이 챕터를 마무리 하기 전에, 온라인 SVM classifier를 빠르게 살펴보자.(온라인 학습은 일반적으로 새로운 인스턴스가 도착할 때 점진적으로 학습한다는 것을 잊지 말자)\n",
    "\n",
    "선형 SVM 분류기에 대한 하나의 방법은 Primal problem로부터 도출된 식 5-13의 비용 함수를 최소화하기 위해 Gradient Descent(eg:SGDClassifier 사용)를 사용하는 것이다. 불행히도 QP를 기반으로 하는 방법보다 훨씬 느리게 수렴한다.\n",
    "\n",
    "![Image](figures/28.png)\n",
    "\n",
    "Cost 함수의 첫 번째 합은 모델이 작은 가중치 벡터 w를 갖도록 밀어 넣어 큰 여백을 만든다. 두 번째 합은 모든 마진 위반의 합계를 계산한다. 거리에서 벗어나 올바른 위치에 있는 경우 인스턴스의 마진 위반은 0과 같거나 그렇지 않으면 거리의 올바른 면까지의 거리에 비례한다. 이 term을 최소화하면 모델이 마진 위반을 가능한 작고 적게 만든다.\n",
    "\n",
    "online kernelized SVM (eg:\"Incremental and Decremental SVM Learning\" or \"Fast Kernel Classifiers with Online and Active Learning\")를 구현하는 것도 가능하다. 그러나 Matlab 및 C ++에서 구현된다. 대규모 비선형 문제에 대해서는 Neural Net을 대신 사용하는 것을 고려할 수도 있다.\n",
    "\n",
    "#### Hinge Loss\n",
    "> The function max(0, 1 – t) is called the hinge loss function (represented below). It is equal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not differentiable at t = 1, but just like for Lasso Regression (see “Lasso Regression” on page 130) you can still use Gradient Descent using any subderivative at t = 0 (i.e., any value between –1 and 0).\n",
    "> ![Image](figures/29.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "471px",
    "right": "19px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
