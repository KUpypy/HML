{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chp14. Recurrent Neural Network 뒷부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 발표자: 이우진\n",
    "* 발표일: 2017.9.2(토)\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Difficulty of Tranining over Many Time Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "긴 시퀀스에서 RNN을 훈련 시키려면 많은 step을 실행해야 한다. 이렇게하면 펼쳐진 RNN이 매우 깊은 네트워크가 된다. 다른 심층 신경망과 마찬가지로 그것은 제 11 장에서 논의된 vanishing / exploding gradient로 인해 훈련이 엄청나게 느릴 수 있다. 이 문제를 해결하기 위해 우리가 논의한 많은 트릭은 파라메터를 잘 설정하는 것, 활성화 함수(ReLU), Batch Normalization, 그라디언트 클리핑 및 더 빠른 옵티 마이저와 같은 것들이고 이는 unrolled RNN에도 사용할 수 있다. 그러나, RNN이 적당히 긴 시퀀스 (예를 들어, 100 개의 입력)를 처리 할 필요가 있다면, 트레이닝은 여전히 매우 느릴 것이다.\n",
    "\n",
    "이 문제에 대한 가장 간단하고 가장 일반적인 해결책은 훈련 중에 제한된 수의 시간 단계에서만 RNN을 언롤링하는 것이다. 이를 _truncated backpropagation through time_ 이라고 한다. TensorFlow에서는 입력 시퀀스를 자르기 만하면 구현할 수 있습니다. 예를 들어, 시계열 예측 문제에서는 n_steps만 줄이면 된다. 여기서 문제는 모델이 장기적인 패턴을 배울 수 없다는 것이다. 한 가지 해결 방법은 단축 된 시퀀스에 이전 데이터와 최근 데이터가 모두 포함되도록 하여 모델에서 두 가지 모두를 사용할 수 있도록 할 수 있다 (예 : 시퀀스에 지난 5 개월 동안 월별 데이터, 지난 5 주 동안 주간 데이터가, 그리고 나서 지난 5 일 동안 일별 데이터). 그러나 이 해결 방법에는 한계가 있다. 작년의 데이터가 실제로 유용하다면 어떨까? 선거 결과와 같은 짧지만 중요한 사건이 있다면 어떻게 될까?\n",
    "\n",
    "오랜 훈련 시간 외에도, long-running RNN이 직면한 또 다른 문제는 첫 번째 인풋의 기억이 점차 사라지는 것이다. 실제로 RNN을 통과 할 때 데이터가 통과하는 변환으로 인해 시간 간격마다 일부 정보가 손실된다. 잠시 후, RNN의 상태는 사실상 첫 번째 입력의 흔적을 포함하지 않게 된다. 예를 들어 \"이 영화 진짜 좋았어.\"라는 긴 리뷰에 대해 감정 분석을 수행하려 한다고 하자. 하지만 나머지 부분에서는 영화를 더욱 좋게 만들 수 있는 많은 것들을 나열한다고 해보자. RNN이 처음 네 단어를 점차 잊어 버리면 리뷰가 완전히 잘못 해석된다. 이 문제를 해결하기 위해 장기 기억을 지닌 다양한 유형의 셀이 도입되었다다. 그들은 너무 성공적이어서 기본 세포가 더 이상 사용되지 않는다는 것을 증명했다. 우선 이 긴 메모리 셀 중에서 가장 인기있는 LSTM 셀을 살펴 보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSTM Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short-Term Memory(LSTEM)셀은 1997년 Sepp Hochreiter와 Jurgen Schmidhuber에 의에 고안되었고, Alex Graves, Hasim Sak, Wojciech Zarembadhk 와 같은 사람들에 의해 향상되었다. LSTM을 블랙 박스처럼 생각하면 이를 베이직 셀처럼 사용 할 수도 있다. 이게 훨씬 나은 것 빼면; 훈련은 훨씬 빨리 converge할 것이고, 긴 기간의 dependencies도 기억할 것이다. 텐서플로우에서는 `BasicLSTMCell`을 쓰면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "n_neurons = 5\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units = n_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM셀은 두개의 상태 벡터를 이용하며, 성능의 이유로 기본적으로 따로 유지된다. 이를 바꾸려면 `BasicLSTEMCell`을 만들때 `state_is_tuple=False`를 이용하면 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch14_1.png\" style=\"width:700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state는 두가지 벡터로 나눠진다: $h_{(t)}$와 $c_{(t)}$(\"c\"는 \"cell\"을 의미).$h_{(t)}$를 짧은 기간의 상태, $c_{(t)}$를 긴 기간의 상태로 생각하면 편하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "중요한 아이디어는 네트워크가 long-term state중 어떤 것을 배울지 학습할 수 있다는 것이다. $c_{(t-1)}$ 가 왼쪽에서 오른쪽으로 이동할때, _forget gate_를 통과하며 조금의 메모리를 소실하고 덧셈(_input gate_에 의해 선택된 정보들을 더함)에 의해 새로운 기억을 더한다. 결과인 $c_{(t)}$는 변형 없이 바로 나간다. 따라서, 각 시간 과정에서 어떤 메모리는 소실되고 어떤 메모리는 더해진다. 거기에 더해서, 덧셈 이후 장기 상태는 복제되어 tanh 함수를 거쳐 _output gate_에 의해 필터링 된다. 이는 곧 단기 상태인 $h_{(t)}$ ($y_{(t)}$와 동치인) 를 만든다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "첫째로, 인풋 벡터 $x_t$ 와 h_(t-1)이 네개의 fully connected layer에 뿌려진다. 각 레이어는 각각 다른 목적을 가진다.\n",
    "\n",
    "* 메인 레이어는 $g_t$를 만든다. 이는 $x_t$와 $h_{(t-1)}$을 분석한다. 베이직 셀에서는 이 레이어만 사용해 바로 $y_t$와 $h_t$를 만들어 낸다. 반대로, LSTM에서는 이 레이어는 바로 나가지 않고 장기 상태로 부분적으로 전이된다.\n",
    "* 다른 세개의 레이어는 gate controllers로, 로지스틱 활성함수를 사용해 0~1사이의 결과같을 낸다. 그들의 결과같은 element-wise 곱 되기 때문에, 0을 뱉어내면 게이트를 닫으며 1을 뱉으면 게이트를 연다. 상세하게 보면\n",
    "    - forgate gate: $f_t$에 의해 컨트롤 되며, 어떤 부분의 장기 상태가 지워져야 할지 결정한다.\n",
    "    - input gate: $i_t$에 의해 컨트롤 되며, $g_t$중 어떤 파트가 장기 상태에 더해져야 하는지 결정한다.\n",
    "    - output gate: $o_t$에 의해 컨트롤 되며, 장기 상태중 어떤 부분이 읽어져야 하는지 결정하고 $h_t$와 $y_t$를 뱉어낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "즉, LSTM 셀은 중요한 인풋을 식별(input gate)하고 장기 상태에 이를 저장할 수 있으며, 이것이 필요한 이상 계속 유지한다(forget gate). 그리고 필요할때마다 뽑아낸다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"ch14_2.png\" style=\"width:500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch14_3.png\" style=\"width:700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peephole Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적인 LSTM 셀에서, 게이트 컨트롤러는 인풋 $x_t$와 이전의 단기 상태 $h_{(t-1)}$만을 볼 수 있다. 그들에게 장기 상태를 보게 하주는 것은 좋은 아이디어이다. 이는 2000년 Felix Gers와 Jurgen Schmidhuber가 제안했다. 그들은  _peephole connections_라는 커넥션을 추가한 LSTM variant를 제안했다: 이전 장기 상태 $c_{(t-1)}$이 fotget gate와 input gate에 인풋처럼 더해지고, 현재 장기 상태 $c_t$는 output gate에 인풋처럼 더해지는 것이 그것이다.\n",
    "\n",
    "peephole 커넥션을 텐서플로우로 구현하려면, 이를 사용해야 한다.\n",
    "\n",
    "`lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Gated Recurrent Unit(GRU)_셀은 조경현 교수가 2014년에 제안했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch14_4.png\" style=\"width:700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 셀은 LSTEM셀의 간소화된 버전인데, 꽤 좋은 성능을 낸다.\n",
    "\n",
    "\n",
    "* 두개의 상태 벡터가 하나의 벡터 $h_t$로 통합되었다.\n",
    "* $r_t$: reset gate - 새로운 입력을 이전 메모리와 어떻게 합칠지\n",
    "* $z_t$: update gate - 이전 메모리를 얼만큼 기억할지 (input gate + forget gate)\n",
    "* 하나의 게이트 컨트롤러가 forget gate와 input gate를 둘다 컨트롤한다. 게이트 컨트롤러가 1을 뱉어내면, input gate는 열리고 forget gate는 닫힌다. 만약 0을 뱉어내면, 반대가 벌어진다. 즉 기억이 저장되어야 할 때마다 기억이 저장되어야 하는 공간이 먼저 지워진다. \n",
    "* output gate가 없다; full state vector가 곧 매 time step의 아웃풋이다. 하지만, 이전 상태의 어느 부분이 메인 레이어에 표시될지를 제어하는 새로운 게이트 컨트롤러가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch14_5.png\" style=\"width:500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_t = y_t =  (g_t*z_t)+(1-z_t)*h_{(t-1)}$ 인데 이해를 잘 못하겠음. 다른 수식을 보면 이럼\n",
    "\n",
    "<img src=\"ch14_6.png\" style=\"width:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 셀은 다음과 같이 만들어 진다.\n",
    "\n",
    "`gru_cell = tf.contrib.rnn.GRUCell(num_units=n_neurons)`\n",
    "\n",
    "LSTEM or GRU셀은 최근 RNN의 성공의 큰 요소중 하나이다. 특히 NLP에서 그렇다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분 최신의 NLP 적용들, 기계 번역, 자동 요약, 파싱, 감정 분석 등은 RNN을 베이스로 한다. 여기서 기계 번역을 잠깐 볼 것이다. 이는 텐서플로우의 Word2Vec이나 Seq2Seq 튜토리얼에 잘 설명되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시작하기 전에 단어 표현을 선택해야 한다. 하나의 옵션은 one-hot 벡터를 사용하여 각 단어를 표현하는 것일 수 있다. 어휘에 50,000 단어가 포함 된 경우 n 번째 단어는 50,000 차원 벡터로 표현되며 n 번째 위치에서 1을 제외하고는 0으로 채워진다. 그러나, 그런 큰 어휘로, spare representation은 전혀 효율적이지 않을 것이다. 이상적인 것은 유사 단어가 비슷한 표현을 가지기를 원하기 때문에 모델이 비슷한 단어에 대해 배운 것을 일반화 하는 것이다. 예를 들어, 모델에 \"나는 우유를 마신다\"라는 문구가 유효한 문장이고, \"우유\"가 \"물\"에 가까우면서 \"신발\"에서 멀다는 것을 알고 있다면 \"나는 물을 마신다\"는 것을 알게 될 것이다. 반면에 \"나는 신발을 마신다\"는 아마도 아닐 것이다. 그럼 어떻게 이를 만들까?\n",
    "\n",
    "가장 보편적인 해결책은 임베딩(embedding)이라고 하는 매우 작고 밀집된 벡터 (예 : 150 차원)를 사용하여 어휘의 각 단어를 표현하고 신경망이 이러한 임베딩을 잘 학습할 수 있도록 하는 것이다. 훈련이 시작될 때, 임베딩은 무작위로 선택되지만, 훈련 중에는 역전파가 신경망이 작업을 수행하는 데 도움이되는 방식으로 임베딩을 자동으로 이동시킨다. 일반적으로 이는 비슷한 단어가 점차 서로 가깝게 뭉치 며 오히려 의미있는 방식으로 구성되는 것을 의미한다. 예를 들어, 임베딩은 성별, 단수 / 복수, 형용사 / 명사 등을 나타내는 다양한 축을 따라 배치 될 수 있다. \n",
    "\n",
    "TensorFlow에서는 먼저 어휘의 모든 단어에 대한 포함을 나타내는 변수를 생성해야 한다(무작위로 초기화 됨)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "embedding_size = 150\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 \"나는 우유를 마신다\"라는 문장을 신경망에 공급한다고 하자. 먼저 문장을 사전 처리하여 알려진 단어 목록으로 분리해야 한다. 예를 들어 불필요한 문자를 제거하고 \"[UNK]\"와 같은 미리 정의 된 토큰 단어로 알수 없는 단어를 대체하거나 숫자 값을 \"[NUM]\"으로 대체하거나 URL을 \"[URL]\"로 바꿀 수 있다. 일단 알려진 단어 목록을 얻으면 사전에서 각 단어의 정수 식별자 (0에서 49999까지)를 찾을 수 있다 (예 : [72, 3335, 288]). 이 시점에서 placeholder를 사용하여 TensorFlow에 이러한 단어 식별자를 제공하고 embedding_lookup() 함수를 적용하여 거기에 알맞은 임베딩을 가져올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_inputs = tf.placeholder(tf.int32, shape=[None]) # from ids...\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs) # ...to embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 좋은 워드 임베딩을 학습하면, NLP 응용 프로그램에서 실제로 효율적으로 재사용 할 수 있다. 어쨌든 \"우유\"는 응용 프로그램이 무엇이든 \"물\"에 가깝고 \"신발\"에서 멀리 떨어져 있기 때문이다. 실제로, 자신의 워드 임베딩을 훈련하는 대신 미리 짜여진 워드 임베딩을 다운로드 할 수 있다. 사전 학습 된 레이어 (11 장 참조)를 다시 사용할 때와 마찬가지로 사전에 포함 된 삽입을 고정하거나 (예 : trainable = False를 사용하여 embeddings 변수를 생성) 또는 응용 프로그램에 맞게 backpropagation을 조정할 수 있다. 첫 번째 옵션을 사용하면 교육 속도가 빨라지지만 두 번째 옵션을 선택하면 약간 높은 성능을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    임베딩은 특히 값 사이에 복잡한 유사성이 있는 경우, 많은 다른 의미를 취할 수 있는 카테고리 속성을 나타내는 데 유용하다. 예를 들어, professions, hobbies, dishes, species, brands 등."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## An Encoder-Decoder Network for Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어를 프랑스어로 바꾸는 간단한 기계 번역 모델을 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"ch14_7.png\" style=\"width:700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어 문장은 인코더로 보내지고 디코더는 프랑스어 번역을 출력한다. 프랑스어 번역은 디코더의 입력으로도 사용되지만 한 단계 씩 뒤로 밀려 나온다. 즉, 디코더는 이전 단계에서 출력해야하는 단어를 입력으로 제공한다(실제 출력 내용에 관계없이). 첫 번째 단어의 경우 문장의 시작을 나타내는 토큰이 제공된다 (예 : \"`<go>`\"). 디코더는 EOS (End-of-sequence) 토큰 (예 : \"`<eos>`\")으로 문장을 끝낼 것으로 예상됩니다.\n",
    "\n",
    "영어 문장은 인코더에 공급되기 전에 거꾸로 뒤집힌다. 예를 들어, \"I drink milk\"는 \"milk drink I\"로 바뀐다. 이렇게하면 영어 문장의 시작 부분이 마지막으로 엔코더에 공급된다. 이는 일반적으로 디코더가 번역해야 하는 첫 번째 항목이기 때문에 유용하다.\n",
    "\n",
    "각 단어는 처음에는 간단한 정수 식별자 (예 : 단어 \"우유\"의 경우 288)로 표시된다. 다음으로, 임베딩 룩업은 임베딩(embedding)이라는 단어를 리턴한다 (앞에서 설명한 바와 같이, 이는 밀도가 높고 상당히 낮은 차원의 벡터이다). 이러한 워드 임베딩은 실제로 인코더와 디코더에 공급되는 것이다.\n",
    "\n",
    "각 단계에서, 디코더는 출력 어휘 (즉, 프랑스어)에서 각 단어에 대한 점수를 출력하고, Softmax 층은 이들 점수를 확률로 전환시킨다. 예를 들어, 첫 번째 단계에서 \"Je\"라는 단어의 확률은 20 %이고 \"Tu\"의 확률은 1 % 이다. 확률이 가장 높은 단어가 출력된다. 이것은 일반적인 분류 작업과 매우 비슷하므로 `softmax_cross_entropy_with_logits()` 함수를 사용하여 모델을 학습 시킬수 있다.\n",
    "\n",
    "추론 시간 (훈련 후)에서는 디코더에 보낼 문장이 없다. 대신, 그림 14-16에 표시된 것처럼 이전 단계에서 출력 한 단어를 디코더에 공급하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch14_8.png\" style=\"width:500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 큰 그림을 그렸다. 하지만, 텐서플로우의 sequence-to-sequence 튜토리얼을 보고 `rnn/translate/seq2seq_model.py`를 보면, 조금 다른 점을 알 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫째, 지금까지 모든 입력 시퀀스 (인코더와 디코더에 대한)가 일정한 길이를 갖는다고 가정했다. 그러나 분명히 문장 길이가 다를 수 있다. 이것을 처리 할 수있는 몇 가지 방법이 있다. 예를 들어, `static_rnn()` 또는 `dynamic_rnn()` 함수에 대한 sequence_length 인수를 사용하여 각 문장의 길이를 지정한다(앞에서 설명한대로). 그러나 다른 방법이이 튜토리얼에서 사용된다 (아마도 성능상의 이유로) : 문장은 유사한 길이의 버켓으로 그룹화된다(예를 들어 1-6 문장의 버킷, 7-12 문장의 문장, 등등), 짧은 문장은 특별한 패딩 토큰 (예 : \"`<pad>`\")을 사용하여 덧붙여진다. 예를 들어 \"I drink milk\"는 \"`<pad> <pad> <p>` milk drink I\"가 되고 \"Je bois du lait `<eos> <pad>`\"가 된다. 물론, 우리는 EOS 토큰을 이후의 출력을 무시하고 싶다. 이를 위해 튜토리얼의 구현에서는 target_weights 벡터를 사용한다. 예를 들어, \"Je bois du lait `<eos> <pad>`\"대상 문장의 경우, 가중치는 [1.0, 1.0, 1.0, 1.0, 0.0]으로 설정된다 (패딩 토큰에 해당하는 가중치 0.0을 확인). 단순히 손실을 표적 가중치로 곱하면 EOS 토큰 이후 단어에 해당하는 손실은 제로가 된다.\n",
    "\n",
    "\n",
    "- 둘째, 출력 어휘가 큰 경우, 가능한 모든 단어에 대해 확률을 출력하는 것은 대단히 느릴 것이다. 목표 어휘에 50,000 개의 프랑스어 단어가 포함되어 있다면 디코더는 50,000 차원의 벡터를 출력하고 그런 큰 벡터에 대해 softmax 함수를 계산하는 것은 매우 큰 계산이다. 이를 피하기 위해서는 해독기가 1,000 차원 벡터와 같은 훨씬 더 작은 벡터를 출력하도록 한 다음 샘플링 기술을 사용하여 손실을 추정하면 된다. 이 Sampled Softmax 기술은 2015 년 Sébastien Jean 등이 도입했다. TensorFlow에서 `sampled_softmax_loss()` 함수를 사용할 수 있다.\n",
    "\n",
    "\n",
    "- 셋째, 튜토리얼의 구현은 디코더가 입력 시퀀스를 들여다 볼 수 있도록 _attention mechanism_을 사용한다. Attention augmented RNN은 이 책의 범위를 벗어나지만, 관심이 있다면 기계 번역, 기계 판독, 이미지 캡션에 대한 유용한 문서가 있다.(책 참고)\n",
    "\n",
    "\n",
    "- 마지막으로 튜토리얼의 구현은 다양한 Encoder-Decoder 모델을 쉽게 구축 할 수있는 도구를 제공하는 `tf.nn.legacy_seq2seq` 모듈을 사용한다. 예를 들어, `embedding_rnn_seq2seq()` 함수는 자동으로 단어 embedding을 처리하는 간단한 EncoderDecoder 모델을 만든다. 그림 14-15에 표시된 것과 같다. 이 코드는 새로운 `tf.nn.seq2seq` 모듈을 사용하도록 빠르게 업데이트 될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
