{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chp16. Reinforcement Learning\n",
    "\n",
    "- __발표자 : 송서하, 정지원__\n",
    "- __발표일 : 2017.9.16(토)__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  RL\n",
    "\n",
    "강화학습은 지금 머신러닝에서 큰 관심을 받는 분야지만, 1950년대 부터 존재하던 오래된 분야이기도 합니다.\n",
    "\n",
    "오랜전부터 강화학습은 게임과 기계컨트롤등의 분야에서 이용되어 왔으나, 큰 주목을 받지 못했습니다.\n",
    "\n",
    "2013년까지는 말이죠!\n",
    "\n",
    "2013년 딥마인드사는 강화학습을 통해 컴퓨터가 바둑을 밑바닥부터 배울 수 있다고 말했습니다.\n",
    "\n",
    "진짜 밑바닥, 게임 규칙과 전략도 없이, 그냥 바둑돌의 위치를 인풋만으로 받아 컴퓨터의 학습을 시작했습니다.\n",
    "\n",
    "2016년 결국 인공지능 '알파고'는 탑급 바둑기사 이세돌 선수를 상대로 승리합니다.\n",
    "\n",
    "###### RL + DL = Deep Reinforcement Learning\n",
    "\n",
    "이전에 바둑에 도전한 그 어떤 인공지능도, 숙련된 바둑선수의 실력 근처에도 가지 못했습니다.\n",
    "\n",
    "알파고의 등장 이후 강화학습은 다시 부활했습니다. 엔지니어들은 여러 분야에서 강화학습의 응용방안을 고민합니다.\n",
    "\n",
    "어떻게 이런일이 가능했을까요?\n",
    "\n",
    "딥마인드는 강화학습과 심층신경망학습을 합쳐 유래없는 인공지능을 만들고야 말았습니다.\n",
    "\n",
    "이제 심층강화학습은 바둑뿐만 아니라 '걷기로봇', '게임', '자동매매', '자율주행' 등 여러 분야에서 연구중입니다.\n",
    "\n",
    "![Image](figures/1_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning to Optimize Rewards\n",
    "\n",
    "### 보상을 최적화 하도록 학습합니다\n",
    "\n",
    "강화학습은 다음 네개의 요소로 설명할 수 있습니다.\n",
    "\n",
    "- 행위자(agent) : 로봇청소기\n",
    "- 관찰(observation) : 로봇에 달린 카메라나 센서 등\n",
    "- 행동(action) : 운동하는 모터로 보내는 신호\n",
    "- 환경(enviroment) : 먼지쌓인 집 \n",
    "\n",
    "**행위자**는 **환경**안에서 **관찰**을 통해 **행동**합니다.\n",
    "\n",
    "행위자의 엔지니어로부터 다음과 같은 임무를 부여받습니다.\n",
    "\n",
    "> 어이! 로봇청소기! 30분안에 최대한 많은 먼지를 제거해\n",
    "\n",
    "> 만족스러울 만큼 먼지를 제거할 수 있을 때까지 계속 반복해\n",
    "\n",
    "행위자는 시행착오를 반복하며 이득이 최대화 되도록 학습을 하게 됩니다.\n",
    "\n",
    "필요에 따라서 이런 명령도 추가해 줄 수 있습니다.\n",
    "\n",
    "> 움직이다가 벽에 부딛히면 너에게 고통을 줄거야\n",
    "\n",
    "이로인해 행위자는 이득은 최대화하며 고통은 덜 받는 방법을 찾게 됩니다.\n",
    "\n",
    "## Policy Search\n",
    "\n",
    "### 정책 탐색\n",
    "\n",
    "**행위자**가 **행위**를 찾기위해 사용하는 알고리즘을 **정책**이라고 부릅니다. \n",
    "\n",
    "![Image](figures/2_.png)\n",
    "\n",
    "위 그림에서는 **행위자**가 **환경**에서 **관찰**한 인풋을 뉴럴 네트워크를 통해 분석후 **행위**합니다.\n",
    "\n",
    "위 그림에서 **정책**은 뉴럴 네트워크입니다.\n",
    "\n",
    "어떤 알고리즘이든 상관 없이 정책으로 선정될 수 있습니다. \n",
    "\n",
    "정책이 확정적인 답을 내지 못해도 괜찮습니다. 로봇청소기가 절반의 확률로 왼쪽이나 오른쪽으로 회전하는 것처럼 말이죠.\n",
    "\n",
    "정책은 여러 단계에 걸쳐서 작동되며 매 단계마다 확률적인 답을 내놓습니다.\n",
    "\n",
    "이런 과정의 정책을 *stochastic policy*라 부릅니다.\n",
    "\n",
    "###### 어떤 정책을 통해 학습시킬것인가\n",
    "\n",
    "로봇청소기의 예시를 통해 생각해 봅시다.\n",
    "\n",
    "로봇청소기의 경우 두가지 정책 파라미터가 있다고 가정해 봅시다. \n",
    "\n",
    "- 회전 방향을 결정하는 확률\n",
    "- 회전 각도\n",
    "\n",
    "![Image](figures/3_.png)\n",
    "\n",
    "\n",
    "정책을 위한 두 파라미터를 찾기위해, 다음처럼 할 수 있습니다.\n",
    " \n",
    "> 가능한 모든 경우를 시험해 보고 최선의 조합을 찾아보기 - *brute force*\n",
    "\n",
    "이 과정을 정책탐색( *policy search* )이라고 합니다.\n",
    "\n",
    "하지만 brute force는 너무 정책범주( *policy space* )가 크다는 문제가 있습니다.\n",
    "\n",
    "이런 식으로 정책을 찾는 것은 사막에서 바늘찾기나 다름 없는 짓입니다.\n",
    "\n",
    "이 알고리즘의 정답은 당신의 증손주도 확인할 수 없을 겁니다.\n",
    "\n",
    "###### GA\n",
    "\n",
    "brute force말고 다른 방법으로, 유전알고리즘(genetic algorithm)을 사용해 볼수 있습니다.\n",
    "\n",
    "1. 100개의 무작위 정책파라미터 조합을 만들고 청소기를 돌립니다.\n",
    "2. 100개중 임무를 잘 수행하지 못한 80개의 조합을 버립니다.\n",
    "3. 남은 20개의 조합에서 각각의 조합이 4개의 자식조합을 만들도록 합니다. (다시 조합은 100개가 되었습니다.)\n",
    "4. 1개의 부모조합에서 나온 4개의 자식조합은 돌연변이처럼 부모와는 약간 다른 조합을 가집니다.\n",
    "5. 좋은 정책을 찾을 때까지 반복합니다.\n",
    "\n",
    "###### gradient\n",
    "\n",
    "다른 방법으로 흔한 최적화 기법을 사용할 수도 있습니다.\n",
    "\n",
    "정책 파라미터가 움직이는 방향에 대한 이득의 변화량을 계산하고 이득이 커지도록 파라미터를 조정할 수 있습니다.\n",
    "\n",
    "이런 방식을 PG(policy gradients)라고 부릅니다.\n",
    "\n",
    "(이쯤에서 류성원님이 말하셨던 MCMC가 재조명 됩니다!)\n",
    "\n",
    "## Introduction to OpenAI Gym\n",
    "\n",
    "강화학습에서 가장 난관중 하나가 바로, 행위자를 학습시킬 환경을 조성하는 것입니다.\n",
    "\n",
    "하지만 현실에서 많은 경우, 반복 학습을 할 환경을 조성하는 것 조차 어렵습니다. (수행 시간까지 고려해서)\n",
    "\n",
    "그래서 시뮬레이션 환경이 필요합니다.\n",
    "\n",
    "바둑으로 예를 들면, 다이소에 가서 미니 바둑판 100개를 사고 마구 둬볼 필요 없이,\n",
    "\n",
    "컴퓨터로 만든 바둑 시뮬레이션 프로그램을 사용해야 합니다.\n",
    "\n",
    "OpenAI gym는 이를 위한 툴킷입니다. (바둑, 보드게임, 2~3차원 물리적 상황들을 위한 시뮬레이션이 마련되어 있습니다.)\n",
    "\n",
    "![Image](figures/4_.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-16 07:03:21,339] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00300803,  0.02485042, -0.0121123 ,  0.04571619])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n",
    "# 주어진 환경에서 취할수 있는 선택의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00251102,  0.22014394, -0.01119798, -0.25076353]), 1.0, False, {})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 1\n",
    "env.step(action)\n",
    "# (관찰, 보상, 게임끝?, 비고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41.0, 38.0, 35.0, 38.0, 49.0, 46.0, 55.0, 56.0, 35.0, 37.0, 37.0, 45.0, 48.0, 43.0, 48.0, 38.0, 34.0, 38.0, 68.0, 38.0, 50.0, 34.0, 33.0, 47.0, 52.0, 40.0, 51.0, 47.0, 40.0, 26.0, 39.0, 49.0, 39.0, 31.0, 35.0, 37.0, 52.0, 38.0, 39.0, 46.0, 38.0, 40.0, 48.0, 41.0, 37.0, 42.0, 52.0, 45.0, 53.0, 31.0, 31.0, 36.0, 37.0, 44.0, 34.0, 50.0, 39.0, 33.0, 36.0, 45.0, 31.0, 39.0, 45.0, 34.0, 25.0, 48.0, 25.0, 39.0, 60.0, 39.0, 47.0, 51.0, 42.0, 39.0, 40.0, 40.0, 53.0, 39.0, 34.0, 49.0, 39.0, 49.0, 25.0, 52.0, 36.0, 56.0, 41.0, 35.0, 40.0, 41.0, 41.0, 40.0, 39.0, 32.0, 44.0, 40.0, 43.0, 47.0, 53.0, 45.0, 31.0, 39.0, 34.0, 36.0, 53.0, 45.0, 53.0, 34.0, 47.0, 47.0, 40.0, 25.0, 39.0, 49.0, 36.0, 56.0, 40.0, 34.0, 39.0, 40.0, 51.0, 41.0, 41.0, 52.0, 41.0, 53.0, 39.0, 56.0, 59.0, 34.0, 46.0, 46.0, 63.0, 50.0, 39.0, 40.0, 41.0, 53.0, 48.0, 62.0, 49.0, 38.0, 41.0, 44.0, 41.0, 42.0, 31.0, 55.0, 38.0, 51.0, 52.0, 34.0, 40.0, 42.0, 45.0, 26.0, 47.0, 52.0, 41.0, 41.0, 39.0, 53.0, 37.0, 26.0, 39.0, 57.0, 40.0, 35.0, 46.0, 60.0, 60.0, 41.0, 38.0, 31.0, 38.0, 41.0, 25.0, 39.0, 46.0, 51.0, 40.0, 42.0, 37.0, 39.0, 58.0, 44.0, 37.0, 52.0, 39.0, 48.0, 50.0, 40.0, 51.0, 41.0, 52.0, 35.0, 39.0, 49.0, 51.0, 61.0, 46.0, 42.0, 52.0, 42.0, 39.0, 43.0, 34.0, 42.0, 41.0, 57.0, 40.0, 41.0, 39.0, 35.0, 52.0, 59.0, 55.0, 44.0, 57.0, 45.0, 46.0, 50.0, 38.0, 39.0, 52.0, 34.0, 49.0, 38.0, 25.0, 40.0, 45.0, 35.0, 38.0, 62.0, 39.0, 47.0, 39.0, 26.0, 34.0, 51.0, 25.0, 35.0, 40.0, 48.0, 46.0, 52.0, 25.0, 40.0, 49.0, 42.0, 36.0, 35.0, 51.0, 41.0, 55.0, 41.0, 51.0, 45.0, 41.0, 44.0, 45.0, 36.0, 57.0, 46.0, 43.0, 40.0, 38.0, 34.0, 41.0, 35.0, 72.0, 36.0, 62.0, 25.0, 45.0, 55.0, 42.0, 37.0, 35.0, 46.0, 38.0, 38.0, 51.0, 32.0, 34.0, 42.0, 48.0, 44.0, 52.0, 47.0, 42.0, 47.0, 51.0, 45.0, 39.0, 39.0, 40.0, 51.0, 61.0, 38.0, 47.0, 39.0, 52.0, 42.0, 36.0, 41.0, 39.0, 41.0, 52.0, 40.0, 48.0, 45.0, 41.0, 51.0, 39.0, 54.0, 52.0, 56.0, 33.0, 56.0, 47.0, 25.0, 25.0, 52.0, 34.0, 40.0, 39.0, 34.0, 47.0, 36.0, 52.0, 26.0, 59.0, 38.0, 64.0, 50.0, 39.0, 32.0, 38.0, 50.0, 48.0, 47.0, 49.0, 47.0, 31.0, 40.0, 60.0, 38.0, 39.0, 46.0, 45.0, 38.0, 45.0, 48.0, 53.0, 45.0, 50.0, 34.0, 49.0, 39.0, 34.0, 35.0, 34.0, 41.0, 39.0, 49.0, 34.0, 25.0, 35.0, 25.0, 35.0, 32.0, 41.0, 45.0, 56.0, 52.0, 46.0, 41.0, 25.0, 46.0, 61.0, 46.0, 34.0, 25.0, 36.0, 35.0, 41.0, 45.0, 60.0, 55.0, 39.0, 39.0, 46.0, 36.0, 40.0, 51.0, 58.0, 43.0, 52.0, 36.0, 53.0, 51.0, 52.0, 38.0, 46.0, 60.0, 49.0, 53.0, 49.0, 35.0, 46.0, 34.0, 42.0, 31.0, 53.0, 35.0, 47.0, 39.0, 39.0, 39.0, 40.0, 35.0, 56.0, 34.0, 40.0, 31.0, 67.0, 52.0, 65.0, 40.0, 52.0, 35.0, 42.0, 56.0, 37.0, 53.0, 50.0, 39.0, 31.0, 35.0, 52.0, 25.0, 39.0, 40.0, 55.0, 34.0, 35.0, 35.0, 25.0, 57.0, 46.0, 42.0, 39.0, 33.0, 36.0, 34.0, 39.0, 32.0, 51.0, 49.0, 50.0, 59.0, 45.0, 25.0, 49.0, 34.0, 35.0, 48.0, 39.0, 36.0, 39.0, 39.0, 42.0, 42.0, 36.0, 52.0, 41.0, 26.0, 45.0, 38.0, 51.0, 45.0, 35.0, 43.0, 39.0, 49.0, 39.0, 24.0, 26.0, 63.0, 25.0, 51.0, 40.0, 59.0, 49.0, 50.0, 31.0, 32.0, 38.0, 55.0]\n"
     ]
    }
   ],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(1000):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)\n",
    "    \n",
    "print(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.732\n",
      "8.69920548096\n",
      "24.0\n",
      "72.0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    np.mean(totals),\n",
    "    np.std(totals),\n",
    "    np.min(totals),\n",
    "    np.max(totals),\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Policies\n",
    "\n",
    "주먹을 불끈 쥐고 신경망으로 정책을 만들어 봅시다. 이세돌을 박살 낼 준비가 되었습니다.\n",
    "\n",
    "행위자의 관찰이 이 신경망의 인풋 노드가 될 것입니다. 마찬가지로 신경망의 아웃풋은 행위가 됩니다.\n",
    "\n",
    "CartPole 환경의 경우, \n",
    "\n",
    "인풋은 *각도*, *각속력*, *속력*, *위치* 라는 4개 차원이므로 인풋 노드는 4개입니다.\n",
    "\n",
    "아웃풋 노드는 한개입니다. 이 한개의 노드가 의미하는 바는 무엇일까요? (깜짝 Quiz 타임!)\n",
    "\n",
    "1. 아웃풋은 왼쪽 또는 오른쪽이라는 바이너리 형태이다.\n",
    "2. 확률 p값이다. (p = P(왼쪽으로갈 확률) = 1 - P(오른쪽으로 갈 확률))\n",
    "\n",
    "![Image](figures/1_.png)\n",
    "\n",
    "![Image](figures/5_.png)\n",
    "\n",
    "정답은 2번입니다.\n",
    "\n",
    "유전알고리즘이 그러하듯, 확정적인 답을 내지 않음으로써 지역최적값에 머무르는 것을 방지합니다.\n",
    "\n",
    "이런 접근방식은 행위자가\n",
    "\n",
    "- 이미 좋다고 알려진 방식을 선택\n",
    "- 새로운 좋은 방식을 탐색\n",
    "\n",
    "두개 선택지 사이에 균형을 갖추도록 합니다.\n",
    "\n",
    "CartPole 환경은 한가지 특징이 있습니다. 환경의 현재 상태 정보만으로 정책을 수행해도 괜찮다는 것입니다. -*Markov Process*\n",
    "\n",
    "CartPole의 입력변수에 속력정보가 없다면 더이상 마코브 과정이 아니게 됩니다. (이전 정보를 통해 속력을 계산해야 하므로)\n",
    "\n",
    "\n",
    "마찬가지로 로봇청소기의 환경도 마코브 과정이 아닐 수 있습니다. (먼지를 한번 닦은 곳을 또 닦으면 의미가 없으니)\n",
    "\n",
    "```python\n",
    "n_inputs = 4\n",
    "    n_hidden = 4\n",
    "    n_outputs = 1\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "    hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu, weights_initializer=initializer)\n",
    "    logits = fully_connected(hidden, n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "    outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "    p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "    action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "```\n",
    "\n",
    "위 처럼 간단하게 신경망을 구축할 수 있습니다.\n",
    "\n",
    "이 신경망을 어떻게 트레이닝 할지 다음에서 설명합니다.\n",
    "\n",
    "## Evaluating Actions: The Credit Assignment Problem\n",
    "\n",
    "앞의 신경망이 간단하다고 해서 신경망의 트레이닝도 다른 신경망처럼 간단할 것이라 생각했다면 오산입니다.\n",
    "\n",
    "지금 상태에서 우리는 교사학습을 진행할 수 없습니다. 각 단계마다 어떤 선택이 최선인지 우리는 알지 못합니다.\n",
    "\n",
    "강화학습에는 교사학습이 가능한 레이블은 없지만 지침이 되어주는 **보상**과 **처벌**이 있습니다.\n",
    "\n",
    "**보상**과 **처벌**은 일반적으로 즉각적이지 못하고 불규칙적입니다.\n",
    "\n",
    "> CartPole환경을 예로 들어 봅시다.\n",
    "\n",
    "> 시뮬레이션을 통해 100번의 스텝을 거쳐갔다고 칩니다. 이때 이 100번의 스텝이 좋은지 나쁜지 어떻게 알 수 있을까요?\n",
    "\n",
    "> 우리가 아는 정보는 100번의 스텝 이후 막대기가 넘어졌다는 사실 뿐입니다.\n",
    "\n",
    "> 하지만 막대기가 100번째의 행위때문에 넘어졌는지 \n",
    "\n",
    "> 아니면, 96번째의 잘못된 행위로 인해 넘어져 가다가 100번째에 넘어진 것인지는 알지 못합니다.\n",
    "\n",
    "이 문제를 *Credit Assignment Problem*이라고 부릅니다.\n",
    "\n",
    "행위자가 보상을 받더라도, 어떤 행위로 인해 보상을 받는지 알 길이 없는 경우가 많습니다.\n",
    "\n",
    "이 문제를 해결하기 위해 많이 쓰이는 전략중 하나로\n",
    "\n",
    "행위를 **행위가 실행된 후 받은 보상들의 합**으로 평가하는 것입니다. \n",
    "\n",
    "나중에 실행될수록 할인율을 적용합니다.\n",
    "\n",
    "다음의 예시로 설명합니다.\n",
    "\n",
    "![Image](figures/6_.png)\n",
    "\n",
    "> 물론, 나쁜 행위로인해 막대기가 걷잡을 수 없이 빠르게 넘어질 때,\n",
    "\n",
    "> 그 후에 좋은 행위들이 이어지게 된다면, 좋은 행위도 나쁜 점수를 받게 됩니다.\n",
    "\n",
    "> 하지만 시뮬레이션을 충분히 돌린다면 결국 제 점수를 찾아갈 것입니다.\n",
    "\n",
    "## Policy Gradients\n",
    "\n",
    "PG(policy gradients)에 대해 앞에서 설명했었습니다.\n",
    "\n",
    "> 정책 파라미터가 움직이는 방향에 대한 보상의 변화량을 계산하고 보상이 커지도록 파라미터를 조정하는 것\n",
    "\n",
    "PG에서 인기있는 알고리즘중 하나가 이번에 소개할 REINFORCE Algorithms 입니다.\n",
    "\n",
    "REINFORCE Algorithms을 적용해봅시다.\n",
    "\n",
    "> ### 1 \n",
    "\n",
    "> 게임을 몇판 플레이해줍니다\n",
    "\n",
    "> 플레이하는동안 각 스텝마다 그레디언트를 만들어 줍니다.\n",
    "\n",
    "> 이때 그레디언트는 **지금의 선택(왼쪽으로 갈 확률 p)**을 더 강화하도록 학습합니다. \n",
    "\n",
    "> 이는 현재 행위가 최선의 행위라고 가정하는 것을 의미합니다.\n",
    "\n",
    "\n",
    "> ### 2\n",
    "\n",
    "> 어느정도 플레이가 되었다면\n",
    "\n",
    "> 각 에피소드에서 각 스텝의 행위에 대한 보상점수를 계산합니다.\n",
    "\n",
    "> ### 3\n",
    "\n",
    "> 이전에 계산했던 그레디언트에 보상점수를 곱합니다.\n",
    "\n",
    "- 계산된 행위의 보상점수가 양수라면,\n",
    "\n",
    "> 이는 이 행위가 잘한 행위임을 뜻하며, 우리가 1에서 설정한 그레디언트가 괜찮다는 뜻입니다.\n",
    "\n",
    "- 계산된 행위의 보상점수가 음수라면,\n",
    "\n",
    "> 이는 이 행위가 잘못된 행위임을 뜻하며, 우리가 1에서 설정한 그레디언트가 잘못되었다는 뜻입니다. (반대라는 뜻)\n",
    "\n",
    "> 음수인 보상점수가 곱해져 그레디언트 방향이 반대가 됩니다. (잘못된 그레디언트가 수정되었습니다.)\n",
    "\n",
    "> ### 4\n",
    "\n",
    "> 모든 에피소드로 부터 얻은 그레디언트들을 평균내줍니다.\n",
    "\n",
    "> 이렇게 얻은 그레디언트는 이제 GD를 위해 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-16 07:04:49,987] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    \n",
    "    n_inputs = 4\n",
    "    n_hidden = 4\n",
    "    n_outputs = 1\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "    hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu, weights_initializer=initializer)\n",
    "    logits = fully_connected(hidden, n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "    outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "    p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "    # 왼쪽으로 갈 확률과 오른쪽으로 갈 확률\n",
    "    action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "    # 두개의 선택지 사이에서 확률적 선택을 해줍니다.\n",
    "    y = 1. - tf.to_float(action)    \n",
    "    # y가 0이라면 왼쪽 이동, 1이라면 오른쪽 이동\n",
    "    learning_rate = 0.01\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    # 비용함수\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    # 그레디언트 알고리즘\n",
    "    grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "    # minimize하지 않고 그레디언트만 구합니다.\n",
    "    gradients = [grad for grad, variable in grads_and_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이렇게 만들어준 그래프는 학습하면서 그래디언트를 만들어가게 됩니다.\n",
    "\n",
    "> 몇번의 시뮬레이션 이후 \n",
    "\n",
    "> 우리는 앞서 3, 4번에서 이야기 했던 작업 (만든 그레디언트를 적용하는 일)이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    \n",
    "    gradient_placeholders = []\n",
    "    grads_and_vars_feed = []\n",
    "    for grad, variable in grads_and_vars:\n",
    "        gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "        gradient_placeholders.append(gradient_placeholder)\n",
    "        grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "    \n",
    "    training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "# 스텝마다의 리워드 계산\n",
    "\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]\n",
    "# 종합 리워드 계산과 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working..\n",
      "working..\n",
      "working..\n",
      "working..\n",
      "working..\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    n_iterations = 25\n",
    "    n_max_steps = 1000\n",
    "    # 혹여나 영원히 학습하는 것을 방지합니다.\n",
    "    n_games_per_update = 5\n",
    "    # 5번의 에피소드로 먼저 그레디언트 구축합니다.\n",
    "    save_iterations = 5\n",
    "    discount_rate = 0.95\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for iteration in range(n_iterations):\n",
    "            all_rewards = []\n",
    "            all_gradients = []\n",
    "            for game in range(n_games_per_update):\n",
    "                current_rewards = []\n",
    "                current_gradients = []\n",
    "                obs = env.reset()\n",
    "                for step in range(n_max_steps):\n",
    "                    action_val, gradients_val = sess.run(\n",
    "                        [action, gradients],\n",
    "                        feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "                    obs, reward, done, info = env.step(action_val[0][0])\n",
    "                    current_rewards.append(reward)\n",
    "                    current_gradients.append(gradients_val)\n",
    "                    if done:\n",
    "                        break\n",
    "                all_rewards.append(current_rewards)\n",
    "                all_gradients.append(current_gradients)            \n",
    "            # 5번의 에피소드를 통해 그레디언트를 구축하는 과정입니다.\n",
    "\n",
    "            all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
    "            feed_dict = {}\n",
    "            for var_index, grad_placeholder in enumerate(gradient_placeholders):\n",
    "                mean_gradients = np.mean(\n",
    "                    [reward * all_gradients[game_index][step][var_index]\n",
    "                        for game_index, rewards in enumerate(all_rewards)\n",
    "                        for step, reward in enumerate(rewards)],\n",
    "                    axis=0)\n",
    "                feed_dict[grad_placeholder] = mean_gradients\n",
    "            # 정책을 구축된 그레디언트로 업데이트 해줍니다.\n",
    "                \n",
    "            sess.run(training_op, feed_dict=feed_dict)\n",
    "            if iteration % save_iterations == 0:\n",
    "                print(\"working..\")\n",
    "                saver.save(sess, \"./my_policy_net_pg.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알파고는 위의 PG 알고리즘을 기반으로 설계되었습니다.\n",
    "\n",
    "알파고는 이에 더해 *Monte Carlo Tree Search*라는 \n",
    "\n",
    "알고리즘을 사용했다고 하니 관심이 있으시다면 찾아보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 서하↑\n",
    "---\n",
    "# 지원↓\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "20세기 초, 수학자 Andrey Markov는 마코프 체인이라고 불리는 no-memory 확률 프로세스를 연구했다. 이러한 프로세스는 고정된 수의 상태를 가지며 각 단계마다 하나의 상태에서 다른 상태로 무작위로 진화한다. 상태 s에서 상태 s'로 진화 할 확률은 고정되어 있으며 과거 상태가 아니라 쌍 (s, s')에만 의존한다. (시스템에는 메모리가 없음).\n",
    "\n",
    "![Image](figures/7_.png)\n",
    "\n",
    "위 그림은 4가지 상태를 갖는 마코프 체인의 예를 보여준다. 프로세스가 s0 상태에서 시작하고 다음 단계에서 그 상태로 남아있을 가능성이 70%라고 가정한다. 결국 그것은 다른 상태가 s0을 가리 키지 않기 때문에 다시 그 상태를 떠나야하며 다시는 돌아 오지 않을 것이다. 상태 s1로 가면 상태 s2(90 % 확률)로 이동 한 다음 상태 s1(100 % 확률)로 즉시 돌아간다. 이 두 상태 사이에서 여러 번 번갈아 나올 수 있지만 결국에는 s3 상태가 되어 영원히 계속된다.(terminal state). 마코프 체인은 아주 다른 역동성을 가질 수 있으며 열역학, 화학, 통계 등에 많이 사용된다.\n",
    "\n",
    "![Image](figures/8_.png)\n",
    "\n",
    "Markov decision process은 1950년대 Richard Bellman에 의해 처음 쓰였다. 마코프 체인과 비슷하지만 twist가 있다. 각 단계에서 에이전트는 가능한 여러 가지 행동 중 하나를 선택할 수 있고, transition 확률은 선택된 행동에 의존한다. 또한 일부 상태 전환은 일부 보상(양수 또는 음수)을 반환하며 에이전트의 목표는 시간이 지남에 따라 보상을 최대화하는 정책을 찾는 것이다.\n",
    "\n",
    "예를 들어, 위 그림에 표시된 MDP는 세 단계의 상태와 각 단계에서 가능한 세 가지 개별 액션을 갖는다. 상태가 s0에서 시작하면 에이전트는 작업 a0, a1 또는 a2 중에서 선택할 수 있다. 만약 a1을 선택하면, 단지 상태 s0에 머물러 있고, 보상도 받지 못한다. 따라서 원하는 경우 영원히 거기에 머물기로 결정할 수 있다. 그러나 a0을 선택하면 +10의 보상을 얻고 70%의 확률로 상태 s0에 남는다. 그러면 가능한 한 많은 보상을 얻기 위해 다시 시도 할 수 있다. 그러나 어느 시점에서 그것은 s1 상태로 끝날 것이다. 상태 s1에는 a0 또는 a1의 두 가지 동작만 있다. 반복적으로 a1을 선택하여 머물러있게 하거나, 상태 s2로 이동하여 -50의 negative 보상을 얻도록 선택할 수 있다. 상태 s3에서는 a1을 취하는 것 외에 다른 선택의 여지가 없으며, 상태 s0로 되돌아 가며 +40의 보상을 얻는다. 이 MDP를 살펴봄으로써 어떤 전략이 시간이 지남에 따라 가장 많은 보상을 얻게 될지 짐작할 수 있는까? 상태 s0에서 그것은 액션 a0이 최선의 선택이며, 상태 s3에서 에이전트는 액션 a1을 취할 수밖에 없다. 그러나 상태 s1에서 에이전트 (a0)을 택할지 (a1)을 택할지는 명확하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman은 실제 최적의 행동을 한다고 가정할 때, 상태에 도달 한 후에 에이전트가 평균적으로 기대할 수 있는 모든 discount된 미래의 보상의 합인 $V*(s)$, 즉 최적의 상태 값을 추정하는 방법을 발견했다. 그는 에이전트가 최적으로 행동하면 Bellman Optimality Equation이 적용된다는 것을 보여주었다. 이 재귀 방정식에 따르면 에이전트가 최적으로 행동하면 현재 상태의 최적값은 하나의 최적 동작을 취한 후 평균으로 얻게 될 보상과 이 동작이 이끌어 낼 수 있는 모든 가능한 다음 상태의 예상 최적값을 합친 것과 같다.\n",
    "\n",
    "# $V^{*}(s) = max_{a}\\Sigma_{s^{′}}T(s,a,s^{′})[R(s,a,s^{′})+\\gamma.V^{*}(s^{′})]$ for all $s$\n",
    "\n",
    "- T(s, a, s′) 는 a를 선택하여 s->s′ 가 이루어질 확률\n",
    "- R(s, a, s′) 는 a를 선택하여 s->s′ 가 되면서 받는 보상\n",
    "- γ 는 discount rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation leads directly to an algorithm that can precisely estimate the optimal state value of every possible state: you first initialize all the state value estimates to zero, and then you iteratively update them using the Value Iteration algorithm (see Equation 16-2). A remarkable result is that, given enough time, these estimates are guaranteed to converge to the optimal state values, corresponding to the optimal policy.\n",
    "\n",
    "이 방정식은 가능한 모든 상태의 최적 상태 값을 정확하게 예측할 수 있는 알고리즘을 직접적으로 유도한다. 먼저 모든 상태 값을 0으로 초기화 한 다음, __Value Iteration algorithm__을 사용하여 반복적으로 업데이트한다. 놀라운 것은 충분한 시간이 주어지면 이러한 추정치가 최적 정책에 해당하는 최적 상태 값으로 수렴된다는 것이다.\n",
    "\n",
    "Equation 16-2. Value Iteration algorithm\n",
    "\n",
    "### $V_{k+1}(s) = max_{a}\\Sigma_{s^{′}}T(s,a,s^{′})[R(s,a,s^{′})+\\gamma.V_{k}(s^{′})]$ for all $s$\n",
    "\n",
    "- $V_{k}(s)$ 는 s 상태의 $k^{th}$번 째 반복에서 추정된 값이다.\n",
    "\n",
    "> 이 알고리즘은 다이나믹 프로그래밍의 한 예이며, 복잡한 문제(이 경우 discount된 미래 보상의 무한한 합계를 추정)를 반복적으로 처리할 수 있는 다루기 쉬운 하위 문제로 분해한다.(이 경우 [평균 보상 + discount된 다음 상태의 value]을 최대화하는 동작을 찾는다.).\n",
    "\n",
    "Knowing the optimal state values can be useful, in particular to evaluate a policy, but it does not tell the agent explicitly what to do. Luckily, Bellman found a very similar algorithm to estimate the optimal state-action values, generally called Q-Values. The optimal Q-Value of the state-action pair (s,a), noted Q*(s,a), is the sum of discounted future rewards the agent can expect on average after it reaches the state s and chooses action a, but before it sees the outcome of this action, assuming it acts optimally after that action.\n",
    "Here is how it works: once again, you start by initializing all the Q-Value estimates to zero, then you update them using the Q-Value Iteration algorithm (see Equation 16-3).\n",
    "\n",
    "최적 상태 값을 아는 것은 특히 정책을 평가하는 데 유용할 수 있지만 에이전트에게 수행할 작업을 명시적으로 알려주지는 않는다. 다행히 Bellman은 일반적으로 Q-values라고하는 최적의 state-action 값을 추정하는 매우 유사한 알고리즘을 발견했다. $Q^{*}(s, a)$로 표시되는 state-action쌍 (s, a)의 최적 Q-Value는 에이전트가 상태 s에 도달한 후 a를 선택할 때 평균적으로 기대할 수 있는 discount된 미래 보상의 합이다. 그러나 행동의 결과를 보기 전 상황이며, 그 행동 후에 최적으로 행동한다고 가정한다.\n",
    "\n",
    "식은 다음과 같다. 모든 Q-Value를 0으로 추정하면서 초기화 한 다음, Q-Value Iteration algorithm을 사용하여 업데이트한다.\n",
    "\n",
    "$Q_{k+1}(s,a) \\leftarrow \\sum_{s^{′}}T(s,a,s^{′})[R(s,a,s^{′}) + \\gamma. max_{a^{′}} Q_{k}(s^{′},a^{′})]$ for all $(s,a)$\n",
    "\n",
    "Once you have the optimal Q-Values, defining the optimal policy, noted π*(s), is trivial: when the agent is in state s, it should choose the action with the highest Q-Value for that state:\n",
    "\n",
    "최적의 Q값을 얻었을 때 $\\pi^{*}(s)$라는 최적의 정책을 정의하는 것은 간단하다. 에이전트가 상태에 있을 때 해당 상태에 대해 가장 높은 Q값을 가진 작업을 선택해야한다.\n",
    "\n",
    "### $\\pi^{*}(s) = argmax_{a} Q{*} (s,a)$\n",
    "\n",
    "MDP로 위 그림의 문제를 해결해보자. 먼저 MDP를 정의할 필요가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan=np.nan # represents impossible actions\n",
    "\n",
    "T = np.array([ # shape=[s, a, s']\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],\n",
    "    [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]],])\n",
    "\n",
    "R = np.array([ # shape=[s, a, s']\n",
    "    [[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "    [[10., 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50.]],\n",
    "    [[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]],])\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-value Iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = np.full((3, 3), -np.inf) # -inf for impossible actions\n",
    "\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0 # Initial value = 0.0, for all possible actions\n",
    "\n",
    "learning_rate = 0.01\n",
    "discount_rate = 0.95\n",
    "n_iterations = 100\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    Q_prev = Q.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q[s, a] = np.sum([T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp])) for sp in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21.88646117,  20.79149867,  16.854807  ],\n",
       "       [  1.10804034,         -inf,   1.16703135],\n",
       "       [        -inf,  53.8607061 ,         -inf]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the optimal policy for this MDP, when using a discount rate of 0.95: in state s0 choose action a0, in state s1 choose action a2 (go through the fire!), and in state s2 choose action a1 (the only possible action). Interestingly, if you reduce the discount rate to 0.9, the optimal policy changes: in state s1 the best action becomes a0 (stay put; don’t go through the fire). It makes sense because if you value the present much more than the future, then the prospect of future rewards is not worth immediate pain.\n",
    "\n",
    "이 MDP에 대한 최적 정책은 discount rate 0.95를 사용하는 경우다. 상태 s0에서 a0을 선택하고 상태 s1에서 a2를 선택하고 상태 s2에서 a1을 선택하면 된다.(불을 지나서...). 흥미롭게도, 할인율을 0.9로 낮추면 최적의 정책이 바뀐다. 상태 s1에서 가장 좋은 행동은 a0(집에 머무르면서 불을 지나치지 않음)이 된다. 현재보다 더 많이 미래를 중요하게 생각한다면 미래의 보상에 대한 전망은 즉각적인 고통이 될 수 없기 때문이라고 생각해볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning and Q-Learning\n",
    "\n",
    "Reinforcement Learning problems with discrete actions can often be modeled as Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s′)), and it does not know what the rewards are going to be either (it does not know R(s, a, s′)). It must experience each state and each transition at least once to know the rewards, and it must experience them multiple times if it is to have a reasonable estimate of the transition probabilities.\n",
    "The Temporal Di erence Learning (TD Learning) algorithm is very similar to the Value Iteration algorithm, but tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general we assume that the agent initially knows only the possible states and actions, and nothing more. The agent uses an exploration policy—for example, a purely random policy-to explore the MDP, and as it progresses the TD Learning algorithm updates the estimates of the state values based on the transitions and rewards that are actually observed (see Equation 16-4).\n",
    "\n",
    "Discrte action을 통한 Reinforcement Learning Problem은 마코프 Decision process로 모델링 될 수 있지만, 에이전트는 시작 시점에서 transition probability[$T(s, a, s^{′})$]가 무엇인지  모르고, 보상[$R(s, a, s^{′})$]도 알지 못한다. 보상을 알기 위해서는 적어도 한 번씩 각 상태와 각 transition을 경험해야 하며 T에 대한 합리적인 추정치를 얻으려면 여러 번 경험해야 한다.\n",
    "\n",
    "_Temporal Difference Learning_(TD Learning) 알고리즘은 Value Iteration Algorithm과 매우 유사하지만 에이전트가 MDP에 대한 부분 지식 만 가지고 있다는 사실을 고려하여 조정된다. 일반적으로 우리는 에이전트가 처음에는 가능한 상태와 동작만 알고 있다고 가정한다. 에이전트는 MDP를 탐색하기 위해 탐색 정책(예:purely random policy)을 사용하고 진행하면서 TD 학습 알고리즘은 실제로 관찰된 transitions 및 rewards를 기반으로 상태 값의 추정을 업데이트한다.\n",
    "\n",
    "- __TD Learning algorithm__\n",
    "\n",
    "> ### $V_{k+1}(s) \\leftarrow (1-\\alpha)V_{k}(s)+\\alpha(r+\\gamma . V_{k}(s^{′})$\n",
    "\n",
    "> $\\alpha$는 학습률(e.g., 0.01)\n",
    "\n",
    ">> TD Learning은 한 번에 하나의 샘플을 처리한다는 점에서 Stochastic Gradient Descent와 많은 유사점을 가지고 있다. SGD와 마찬가지로 점진적으로 학습 속도를 낮추면 Optimum으로 수렴 할 수 있다.(그렇지 않으면 최적 근처에서 계속 움직인다).\n",
    "\n",
    "For each state s, this algorithm simply keeps track of a running average of the imme‐ diate rewards the agent gets upon leaving that state, plus the rewards it expects to get later (assuming it acts optimally).\n",
    "Similarly, the Q-Learning algorithm is an adaptation of the Q-Value Iteration algo‐ rithm to the situation where the transition probabilities and the rewards are initially unknown (see Equation 16-5).\n",
    "\n",
    "각 상태에 대해 이 알고리즘은 에이전트가 해당 상태를 떠날 때 얻는 즉시 보상과 나중에 얻을 수있는 보상(최적으로 act한다고 가정)의 평균을 추적한다.\n",
    "\n",
    "Q-Learning 알고리즘은 transition 확률과 보상이 처음에 알려지지 않은 상황에 Q값 반복 알고리즘을 적용한 것이다.\n",
    "\n",
    "- __Q_Learning algorithm__\n",
    "\n",
    "> ### $Q_{k+1}(s,a) \\leftarrow (1-\\alpha)Q_{k}(s,a) + \\alpha(r+\\gamma . max_{a^{′}}Q_{k}(s^{k}, a^{′}))$\n",
    "\n",
    "\n",
    "For each state-action pair (s, a), this algorithm keeps track of a running average of the rewards r the agent gets upon leaving the state s with action a, plus the rewards it expects to get later. Since the target policy would act optimally, we take the maximum of the Q-Value estimates for the next state.\n",
    "\n",
    "각 state-action 쌍 (s, a)에 대해, 이 알고리즘은 a를 통해 상태 s를 떠나는 시점에 에이전트가 얻는 보상의 평균과 나중에 얻을 것으로 예상되는 보상을 더한 값의 경로를 추적한다. 목표 정책이 최적으로 act하기 때문에, 다음 상태를 위해서는 Q-value가 가장 큰 추정을 따르면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.random as rnd\n",
    "\n",
    "learning_rate0 = 0.05\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations = 20000\n",
    "\n",
    "s = 0 # start in state 0\n",
    "\n",
    "Q = np.full((3, 3), -np.inf) # -inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0 # Initial value = 0.0, for all possible actions\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    a = rnd.choice(possible_actions[s]) # choose an action (randomly)\n",
    "    sp = rnd.choice(range(3), p=T[s, a]) # pick next state using T[s, a]\n",
    "    reward = R[s, a, sp]\n",
    "    learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)\n",
    "    Q[s, a] = learning_rate * Q[s, a] + (1 - learning_rate) * (reward + discount_rate * np.max(Q[sp]))\n",
    "    s = sp # move to next state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "충분한 반복을 감안할 때, 이 알고리즘은 최적의 Q값에 수렴할 것이다. train되고 있는 정책이 execute되고 있는 정책가 아니기 때문에 이것을 off-policy 알고리즘이라고 부른다. 이 알고리즘이 에이전트가 무작위로 행동하는 것을 보면서(술취한 원숭이한테 골프를 배우는 경우) 최적의 정책을 학습 할 수 있다는 것은 다소 놀라운 일이다. 더 잘 할 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration Policies\n",
    "\n",
    "Of course Q-Learning can work only if the exploration policy explores the MDP thoroughly enough. Although a purely random policy is guaranteed to eventually visit every state and every transition many times, it may take an extremely long time to do so. Therefore, a better option is to use the ε-greedy policy: at each step it acts randomly with probability ε, or greedily (choosing the action with the highest Q- Value) with probability 1-ε. The advantage of the ε-greedy policy (compared to a completely random policy) is that it will spend more and more time exploring the interesting parts of the environment, as the Q-Value estimates get better and better, while still spending some time visiting unknown regions of the MDP. It is quite com‐ mon to start with a high value for ε (e.g., 1.0) and then gradually reduce it (e.g., down to 0.05).\n",
    "\n",
    "Alternatively, rather than relying on chance for exploration, another approach is to encourage the exploration policy to try actions that it has not tried much before. This can be implemented as a bonus added to the Q-Value estimates, as shown in Equation 16-6.\n",
    "\n",
    "물론 Q-Learning은 탐사 정책이 MDP를 충분히 철저히 탐구할 때만 작동 할 수 있다. 순전히 무작위적인 정책이 궁극적으로 모든 상태와 모든 transition을 여러 번 방문하도록 보장하지만, 그렇게 하면 오랜 시간이 걸릴 수 있다. 따라서 ε-greedy 정책을 사용하는 것이 더 좋다. 각 단계에서 확률 ε으로 무작위로 행동하거나 확률 1-ε로 탐욕스럽게(가장 높은 Q값을 갖는 action 선택) act한다. ε-greedy 정책의 이점(완전히 무작위적인 정책과 비교할 때)은 Q-Value 추정치가 더 좋아지고 더 나아질 때, 환경의 흥미로운 부분을 탐험하는데 점점 더 많은 시간을 소비하면서 MDP의 알려지지 않은 지역을 방문하는 데 약간의 시간을 소비한다는 점이다. ε(e.g.:1.0)에 대한 높은 값으로 시작한 다음 점진적으로 낮춘다.(e.g.: down to 0.05)\n",
    "\n",
    "대안으로, chance에 의존하기보다는 exploration policy가 이전에 많이 시도하지 않은 활동을 시도하도록 장려하는 것이다. 이는 아래 식에 표시된 바와 같이, Q값 추정에 보너스로 추가 구현할 수 있다.\n",
    "\n",
    "> ### $Q(s,a) \\leftarrow (1-\\alpha)Q(s,a) + \\alpha\\lgroup r+\\gamma . max_{\\alpha^{′}} f(Q(s^{′},a^{′}, N(s^{′},a^{′}))\\rgroup$\n",
    "\n",
    "- $N(s^{′}, a^{′})$ counts the number of times the action $a^{′}$ was chosen in state $s^{′}$.\n",
    "- $f(q, n)$ is an exploration function, such as $f(q, n) = q + K/(1 + n)$, where $K$ is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Q-Learning\n",
    "\n",
    "Q-Learning의 주된 문제점은 많은 상태와 action이 있는 대규모(또는 심지어 중간) MDP에 맞게 확장되지 않는다는 것이다. Q-Learning을 사용하여 Ms.Pack-Man을 플레이하는 agent을 학습시키는 것을 고려해보자. Ms. Pac-Man이 먹을 수있는 pellet는 250개가 넘는다. 각각은 존재하거나 아닐 수 있다. (즉, 이미 먹었을 수도 있다). 따라서 가능한 상태의 수는 $2^{250} ≈ 10^{1075}$보다 크다. (이 값은 pellet의 가능한 상태만을 고려한 값). 이것은 관측 가능한 우주의 원자보다 더 많은 방법이므로, 모든 단일 Q-Value에 대한 추정치를 추적할 수 있는 방법은 없다.\n",
    "\n",
    "해결책은 관리 가능한 수의 매개 변수를 사용하여 Q값을 근사화하는 함수를 찾는 것이다. 이를 Approximate Q-Learning이라고합니다. 수년 동안 상태 값(예 : 가장 가까운 유령의 거리, 방향 등)에서 추출한 수작업으로 만들어진 특징을 선형 조합으로 사용하여 Q 값을 추정하는 것이 좋았지만, DeepMind는 Deep-Neural Network를 사용하면 효과가 있음을 보여주었다. 특히 복잡한 문제의 경우 훨씬 더 우수하며 feature 엔지니어링이 필요하지 않다. Q값을 추정하는 데 사용되는 DNN을 DQN(Deep Q-network)이라고하며, Approximate Q-Learning에 DQN을 사용하는 것을 Deep Q-Learning이라고 한다.\n",
    "\n",
    "이 장의 나머지 부분에서는 DeepMind가 2013년에 했던 것처럼 Deep Q-Learning을 사용하여 Pac-Man이 게임을 하도록 에이전트를 교육할 것이다. Atari 게임의 대부분을 아주 잘 수행하는 방법을 배우기 위해 코드를 쉽게 수정할 수 있다. 대부분의 액션 게임에서 초인적인 기술을 얻을 수는 있지만 장기 실행 스토리 라인이있는 게임에서는 그렇게 좋진 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Play Ms. Pac-Man Using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-16 05:38:22,590] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "obs.shape # [height, width, channels] (210, 160, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](figures/9_.png)\n",
    "\n",
    "보다시피 조이스틱의 가능한 9 가지 위치 (왼쪽, 오른쪽, 위, 아래, 가운데, 왼쪽 위 등)에 해당하는 9개의 개별 동작이 있으며 관측치는 Atari의 스크린 샷이다. 화면은 3D NumPy 배열로 표시된다. 이 이미지는 약간 크기 때문에 이미지를 자르고 88x80 픽셀로 축소하고 회색 음영으로 변환하고 Pacman의 대비를 향상시키는 작은 전처리 기능을 만든다. 이렇게하면 DQN에 필요한 계산량이 줄어들고 학습 속도가 빨라진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mspacman_color = np.array([210, 164, 74]).mean()\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # crop and downsize\n",
    "    img = img.mean(axis=2) # to greyscale\n",
    "    img[img==mspacman_color] = 0 # improve contrast\n",
    "    img = (img - 128) / 128 - 1 # normalize from -1. to 1.\n",
    "    return img.reshape(88, 80, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 DQN을 만든다. 그것은 단지 state-action 쌍 (s, a)을 입력으로 취할 수 있고, 해당 Q-Value Q(s, a)의 추정치를 출력 할 수 있지만, action이 discrete하기 때문에, 입력으로 상태 s만을 취하고 action 당 하나의 Q값 추정치를 출력하는 신경망을 사용하는 것이 더 편리하다. DQN은 3개의 convolutional layers와 output layer를 포함한 2개의 FC layers로 구성된다.\n",
    "\n",
    "![Image](figures/10_.png)\n",
    "\n",
    "보다시피, 우리가 사용하게 될 학습 알고리즘은 아키텍처가 같지만 매개 변수가 다른 두 개의 DQN이 필요하다. 하나는 학습 중에 actor 팩맨을 움직이는데 사용되며(배우), 다른 하나는 actor의 시련과 실수를 관찰하며 학습한다.(critic) 규칙적인 간격으로 critic를 actor에게 복사합니다. 두 개의 동일한 DQN이 필요하므로 q_network() 함수를 만들어 이를 빌드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import convolution2d, fully_connected\n",
    "import tensorflow as tf\n",
    "\n",
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "conv_n_maps = [32, 64, 64]\n",
    "conv_kernel_sizes = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4, 2, 1]\n",
    "conv_paddings = [\"SAME\"]*3\n",
    "conv_activation = [tf.nn.relu]*3\n",
    "n_hidden_in = 64 * 11 * 10 # conv3 has 64 maps of 11x10 each\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n # 9 discrete actions are available \n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def q_network(X_state, scope):\n",
    "    prev_layer = X_state\n",
    "    conv_layers = []\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        for n_maps, kernel_size, stride, padding, activation in zip(conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "                                                                    conv_paddings, conv_activation):\n",
    "            prev_layer = convolution2d(prev_layer, num_outputs=n_maps, kernel_size=kernel_size, stride=stride,\n",
    "                                       padding=padding, activation_fn=activation, weights_initializer=initializer)\n",
    "            conv_layers.append(prev_layer)\n",
    "        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n",
    "        hidden = fully_connected(last_conv_layer_flat, n_hidden, activation_fn=hidden_activation,\n",
    "                                 weights_initializer=initializer)\n",
    "        outputs = fully_connected(hidden, n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this code defines the hyperparameters of the DQN architecture. Then the q_network() function creates the DQN, taking the environment’s state X_state as input, and the name of the variable scope. Note that we will just use one observation to represent the environment’s state since there’s almost no hidden state (except for blinking objects and the ghosts’ directions).\n",
    "\n",
    "The trainable_vars_by_name dictionary gathers all the trainable variables of this DQN. It will be useful in a minute when we create operations to copy the critic DQN to the actor DQN. The keys of the dictionary are the names of the variables, stripping the part of the prefix that just corresponds to the scope’s name. It looks like this:\n",
    "\n",
    "먼저 DQN 아키텍처의 하이퍼 파라미터를 정의한다. 그런 다음 q_network() 함수는 환경의 상태 X_state를 입력으로 사용하여 변수 범위의 이름을 사용하여 DQN을 만든다. 숨겨진 상태가 거의 없으므로(점멸하는 객체 및 유령의 방향 제외) 환경의 상태를 나타내기 위해 하나의 관찰만 사용한다.\n",
    "\n",
    "trainable_vars_by_name 사전은 이 DQN의 학습 가능한 모든 변수를 수집한다. critic DQN을 actor DQN에 복사하는 작업을 만들면 유용할 것이다. 사전의 키는 범위 이름에 해당하는 접두어 부분을 제거한 변수의 이름이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channels])\n",
    "actor_q_values, actor_vars = q_network(X_state, scope=\"q_networks/actor\")\n",
    "critic_q_values, critic_vars = q_network(X_state, scope=\"q_networks/critic\")\n",
    "\n",
    "copy_ops = [actor_var.assign(critic_vars[var_name]) for var_name, actor_var in actor_vars.items()]\n",
    "\n",
    "copy_critic_to_actor = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 입력으로 환경 상태(즉, 전처리 된 관찰)를 취할 수 있고, 그 상태에서 각 가능한 동작에 대한 추정된 Q 값을 출력할 수 있는 두 개의 DQN을 갖게 되었다. 또한 critic DQN의 모든 학습 가능한 변수를 actor DQN에 복사하는 copy_critic_to_actor라는 연산이 있다. TensorFlow의 tf.group()함수를 사용하여 모든 할당 작업을 하나의 편리한 작업으로 그룹화한다.\n",
    "\n",
    "actor DQN을 사용하여 Ms. Pac-Man을 재생할 수 있다.(처음에는 매우 안 좋았다) 앞에서 설명한 것처럼 게임을 철저히 탐구하기를 원하기 때문에 일반적으로 게임을 ε-greedy 정책 또는 다른 탐색 전략과 결합하기를 원한다.\n",
    "\n",
    "critic DQN은 게임을 어떻게 배우는가? 간단히 말해 Q-Value 예측을 게임 경험을 통해 actor가 예상한 Q값과 일치 시키려고 한다. 특히 actor를 잠시 놀게하고 모든 경험을 재생 메모리에 저장한다. 각 메모리는 5-tuple(상태, 동작, 다음 상태, 보상, 계속)이되며, 게임이 끝나면 \"계속\"항목은 0.0이거나 그렇지 않으면 1.0이다. 다음으로, 일정한 간격으로 재생 메모리에서 일련의 메모리를 샘플링하고 이러한 메모리에서 Q값을 추정한다. 마지막으로 평소 지도 학습 기법을 사용하여 critic DQN에게 이러한 Q값을 예측하도록 학습 할 것이다. 학습의 반복이 거의 없으면 평론가 DQN을 배우 DQN에 복사한다. 그게 전부다. 아래 식은 critic DQN을 학습시키는데 사용된 비용 함수를 보여준다.\n",
    "\n",
    "- Deep Q-Learning cost function\n",
    "> ### $J(\\theta_{critic}) = {1\\over m}\\Sigma^{m}_{i=1}(y^{i} - Q(s^{i}, a^{i}, \\theta_{critic}))^{2}$ with $y^{(i)} = r^{(i)} + \\gamma . max_{a^{′}}Q(s^{′(i)}, a^{′}, \\theta_{actor})$\n",
    "- $s^{(i)}, a^{(i)}, r^{(i)}$ and $s^{′(i)}$ are respectively the state, action, reward, and next state of the $i^{th}$ memory sampled from the replay memory.\n",
    "- $m$ is the size of the memory batch.\n",
    "- $\\theta_{critic}$ and $\\theta_{actor}$ are the critic and the actor’s parameters.\n",
    "- $Q(s^{(i)},a^{(i)},θ_{critic})$ is the critic DQN’s prediction of the $i^{th}$ memorized state-action’s Q- Value.\n",
    "- $Q(s^{′(i)}, a^{′}, θ_{actor})$ is the actor DQN’s prediction of the Q-Value it can expect from the next state $s^{′(i)}$ if it chooses action $a^{′}$.\n",
    "- $y^{(i)}$ is the target Q-Value for the ith memory. Note that it is equal to the reward actually observed by the actor, plus the actor’s prediction of what future rewards it should expect if it were to play optimally (as far as it knows).\n",
    "- $J(θ_{critic})$ is the cost function used to train the critic DQN. As you can see, it is just the Mean Squared Error between the target Q-Values $y^{(i)}$ as estimated by the actor DQN, and the critic DQN’s predictions of these Q-Values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 재생 메모리(replay memory)는 선택 사항이지만 권장된다. 그것이 없다면, 당신은 critic DQN이 매우 correlated할 수 있는 연속적인 경험을 사용하여 학습시킬 것이다. 이렇게 하면 많은 편향이 생기고 학습 알고리즘의 수렴 속도가 느려진다. 재생 메모리를 사용하여 학습 알고리즘에 공급된 메모리가 상당히 상관 관계가 없음을 보장한다.\n",
    "\n",
    "critic DQN의 학습 작업을 추가한다. 첫째, 메모리 배치에서 각 state-action에 대해 예측된 Q값을 계산할 수 있어야 한다. DQN은 가능한 모든 action에 대해 하나의 Q-Value를 출력하므로 실제로 이 메모리에서 선택된 a에 해당하는 Q-Value 만 유지해야 한다. 이를 위해 a을 one-hot vector로 변환하고, 이를 Q-value로 곱한다. 이것은 memorize된 동작에 해당하는 값을 제외한 모든 Q값을 0으로 만든다. 그런 다음 첫 번째 축을 합하여 각 메모리에 대해 원하는 Q-Value 예측만 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "q_value = tf.reduce_sum(critic_q_values * tf.one_hot(X_action, n_outputs), axis=1, keep_dims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let’s add the training operations, assuming the target Q-Values will be fed through a placeholder. We also create a nontrainable variable called global_step. The optimizer’s minimize() operation will take care of incrementing it. Plus we cre‐ ate the usual init operation and a Saver.\n",
    "\n",
    "다음으로 target Q-Values가 placeholder를 통해 제공된다고 가정하고 학습 operations을 추가해보겠다. global_step이라는 nontrainable 변수를 만든다. Optimizer의 minimize() 연산은 그것을 증가시키는 작업을 처리 할 것이다. 또한 일반적인 init 작업과 Saver를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-63d9b978b434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'global_step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "cost = tf.reduce_mean(tf.square(y - q_value))\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 건설 단계다. 실행 단계를 살펴보기 전에 몇 가지 도구가 필요하다. 먼저, 재생 메모리를 구현하여 시작한다. 최대 메모리 크기에 도달하면 항목을 대기열로 밀어 넣고 목록의 끝에서 꺼내는 것이 매우 효율적이기 때문에 deque를 사용한다. 또한 재생 메모리에서 일련의 경험을 임의로 샘플링하는 작은 함수를 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 10000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memories(batch_size):\n",
    "    indices = rnd.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps_min = 0.05\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if rnd.rand() < epsilon:\n",
    "        return rnd.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values) # optimal action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 100000 # total number of training steps\n",
    "training_start = 1000 # start training after 1,000 game iterations training_interval = 3 # run a training step every 3 game iterations save_steps = 50 # save the model every 50 training steps\n",
    "copy_steps = 25 # copy the critic to the actor every 25 training steps discount_rate = 0.95\n",
    "skip_start = 90 # skip the start of every game (it's just waiting time) batch_size = 50\n",
    "iteration = 0 # game iterations\n",
    "checkpoint_path = \"./my_dqn.ckpt\"\n",
    "done = True # env needs to be reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if done: # game over, start again\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start): # skip the start of each game\n",
    "                obs, reward, done, info = env.step(0)\n",
    "            state = preprocess_observation(obs)\n",
    "            # Actor evaluates what to do\n",
    "            q_values = actor_q_values.eval(feed_dict={X_state: [state]})\n",
    "            action = epsilon_greedy(q_values, step)\n",
    "            \n",
    "            # Actor plays\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            next_state = preprocess_observation(obs)\n",
    "            \n",
    "            # Let's memorize what just happened\n",
    "            replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "            state = next_state\n",
    "            if iteration < training_start or iteration % training_interval != 0:\n",
    "                continue\n",
    "            \n",
    "            # Critic learns\n",
    "            X_state_val, X_action_val, rewards, X_next_state_val, continues = (sample_memories(batch_size))\n",
    "            next_q_values = actor_q_values.eval(feed_dict={X_state: X_next_state_val})\n",
    "            max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "            y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "            training_op.run(feed_dict={X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "            \n",
    "            # Regularly copy critic to actor\n",
    "            if step % copy_steps == 0:\n",
    "                copy_critic_to_actor.run()\n",
    "            \n",
    "            # And save regularly\n",
    "            if step % save_steps == 0: \n",
    "                saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "체크 포인트 파일이 있는 경우 모델을 복원하거나 다른 방법으로 변수를 정상적으로 초기화한다. 그런 다음 메인 루프가 시작된다. 여기서 반복은 프로그램 시작 이후 진행된 게임 단계의 총 수를 계산하고 단계는 교육이 시작된 이후의 총 학습 단계 수를 계산한다. (checkpoint가 restore되면, global step도 복원) 그런 다음 코드가 게임을 reset한다. (아무 것도 일어나지 않는 첫 번째 지루한 게임 단계를 건너 뛴다). 다음으로, actor는 무엇을 해야할지 평가하고 게임을 하며, 그 경험은 재생 메모리에 기억된다. 그런 다음 정기적으로 (워밍업 기간 후) critic는 훈련 단계를 거친다. 그것은 일련의 기억을 샘플링하고 actor에게 다음 상태에 대한 모든 동작의 Q-Values를 추정하도록 요청하고, target QValue y_val을 계산한다. 여기서 유일하게 까다로운 부분은, 게임이 끝난 memory에 해당하는 QValues를 zero로 만들기 위해 다음 상태의 QValues에 연속 벡터를 곱해야 한다는 것이다. 다음으로 우리는 QValues를 예측할 수있는 critic의 능력을 향상시키기 위한 학습 작업을 실시한다. 마지막으로 regular한 간격으로 critic를 actor에게 복사하고 모델을 저장한다.\n",
    "\n",
    "> 불행히도 학습은 매우 느리다. 노트북을 사용하면 PacMan이 좋은 결과를 얻는 데 며칠이 걸릴 것이다. 학습 곡선을 보고 에피소드 당 평균 보상을 측정하면 매우 noisy 하다는 것을 알 수 있다. 어떤 시점에서 갑자기 에이전트가 합리적인 시간 동안 생존한다는 것을 알게 될 때까지는 오랜 시간 동안 명백한 진전이 없을 수 있다. 앞에서 언급했듯이 가능한 많은 사전 지식을 모델에 사전작업(예 : 전처리, 보상 등)하여 주입하고 기본 전략을 모방하기 위해 먼저 교육하여 모델을 부트 스트랩 하면 좋다. 어쨌든 RL은 여전히 많은 인내와 조정이 필요하지만 최종 결과는 매우 흥미롭다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
