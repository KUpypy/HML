{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chp16. Reinforcement Learning\n",
    "\n",
    "- __발표자 : 송서하, 정지원__\n",
    "- __발표일 : 2017.9.16(토)__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  RL\n",
    "\n",
    "강화학습은 지금 머신러닝에서 큰 관심을 받는 분야지만, 1950년대 부터 존재하던 오래된 분야이기도 합니다.\n",
    "\n",
    "오랜전부터 강화학습은 게임과 기계컨트롤등의 분야에서 이용되어 왔으나, 큰 주목을 받지 못했습니다.\n",
    "\n",
    "2013년까지는 말이죠!\n",
    "\n",
    "2013년 딥마인드사는 강화학습을 통해 컴퓨터가 바둑을 밑바닥부터 배울 수 있다고 말했습니다.\n",
    "\n",
    "진짜 밑바닥, 게임 규칙과 전략도 없이, 그냥 바둑돌의 위치를 인풋만으로 받아 컴퓨터의 학습을 시작했습니다.\n",
    "\n",
    "2016년 결국 인공지능 '알파고'는 탑급 바둑기사 이세돌 선수를 상대로 승리합니다.\n",
    "\n",
    "###### RL + DL = Deep Reinforcement Learning\n",
    "\n",
    "이전에 바둑에 도전한 그 어떤 인공지능도, 숙련된 바둑선수의 실력 근처에도 가지 못했습니다.\n",
    "\n",
    "알파고의 등장 이후 강화학습은 다시 부활했습니다. 엔지니어들은 여러 분야에서 강화학습의 응용방안을 고민합니다.\n",
    "\n",
    "어떻게 이런일이 가능했을까요?\n",
    "\n",
    "딥마인드는 강화학습과 심층신경망학습을 합쳐 유래없는 인공지능을 만들고야 말았습니다.\n",
    "\n",
    "이제 심층강화학습은 바둑뿐만 아니라 '걷기로봇', '게임', '자동매매', '자율주행' 등 여러 분야에서 연구중입니다.\n",
    "\n",
    "![Image](figures/1_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Optimize Rewards\n",
    "\n",
    "### 보상을 최적화 하도록 학습합니다\n",
    "\n",
    "강화학습은 다음 네개의 요소로 설명할 수 있습니다.\n",
    "\n",
    "- 행위자(agent) : 로봇청소기\n",
    "- 관찰(observation) : 로봇에 달린 카메라나 센서 등\n",
    "- 행동(action) : 운동하는 모터로 보내는 신호\n",
    "- 환경(enviroment) : 먼지쌓인 집 \n",
    "\n",
    "**행위자**는 **환경**안에서 **관찰**을 통해 **행동**합니다.\n",
    "\n",
    "행위자의 엔지니어로부터 다음과 같은 임무를 부여받습니다.\n",
    "\n",
    "> 어이! 로봇청소기! 30분안에 최대한 많은 먼지를 제거해\n",
    "\n",
    "> 만족스러울 만큼 먼지를 제거할 수 있을 때까지 계속 반복해\n",
    "\n",
    "행위자는 시행착오를 반복하며 이득이 최대화 되도록 학습을 하게 됩니다.\n",
    "\n",
    "필요에 따라서 이런 명령도 추가해 줄 수 있습니다.\n",
    "\n",
    "> 움직이다가 벽에 부딛히면 너에게 고통을 줄거야\n",
    "\n",
    "이로인해 행위자는 이득은 최대화하며 고통은 덜 받는 방법을 찾게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search\n",
    "\n",
    "### 정책 탐색\n",
    "\n",
    "**행위자**가 **행위**를 찾기위해 사용하는 알고리즘을 **정책**이라고 부릅니다. \n",
    "\n",
    "![Image](figures/2_.png)\n",
    "\n",
    "위 그림에서는 **행위자**가 **환경**에서 **관찰**한 인풋을 뉴럴 네트워크를 통해 분석후 **행위**합니다.\n",
    "\n",
    "위 그림에서 **정책**은 뉴럴 네트워크입니다.\n",
    "\n",
    "어떤 알고리즘이든 상관 없이 정책으로 선정될 수 있습니다. \n",
    "\n",
    "정책이 확정적인 답을 내지 못해도 괜찮습니다. 로봇청소기가 절반의 확률로 왼쪽이나 오른쪽으로 회전하는 것처럼 말이죠.\n",
    "\n",
    "정책은 여러 단계에 걸쳐서 작동되며 매 단계마다 확률적인 답을 내놓습니다.\n",
    "\n",
    "이런 과정의 정책을 *stochastic policy*라 부릅니다.\n",
    "\n",
    "###### 어떤 정책을 통해 학습시킬것인가\n",
    "\n",
    "로봇청소기의 예시를 통해 생각해 봅시다.\n",
    "\n",
    "로봇청소기의 경우 두가지 정책 파라미터가 있다고 가정해 봅시다. \n",
    "\n",
    "- 회전 방향을 결정하는 확률\n",
    "- 회전 각도\n",
    "\n",
    "![Image](figures/3_.png)\n",
    "\n",
    "\n",
    "정책을 위한 두 파라미터를 찾기위해, 다음처럼 할 수 있습니다.\n",
    " \n",
    "> 가능한 모든 경우를 시험해 보고 최선의 조합을 찾아보기 - *brute force*\n",
    "\n",
    "이 과정을 정책탐색( *policy search* )이라고 합니다.\n",
    "\n",
    "하지만 brute force는 너무 정책범주( *policy space* )가 크다는 문제가 있습니다.\n",
    "\n",
    "이런 식으로 정책을 찾는 것은 사막에서 바늘찾기나 다름 없는 짓입니다.\n",
    "\n",
    "이 알고리즘의 정답은 당신의 증손주도 확인할 수 없을 겁니다.\n",
    "\n",
    "###### GA\n",
    "\n",
    "brute force말고 다른 방법으로, 유전알고리즘(genetic algorithm)을 사용해 볼수 있습니다.\n",
    "\n",
    "1. 100개의 무작위 정책파라미터 조합을 만들고 청소기를 돌립니다.\n",
    "2. 100개중 임무를 잘 수행하지 못한 80개의 조합을 버립니다.\n",
    "3. 남은 20개의 조합에서 각각의 조합이 4개의 자식조합을 만들도록 합니다. (다시 조합은 100개가 되었습니다.)\n",
    "4. 1개의 부모조합에서 나온 4개의 자식조합은 돌연변이처럼 부모와는 약간 다른 조합을 가집니다.\n",
    "5. 좋은 정책을 찾을 때까지 반복합니다.\n",
    "\n",
    "###### gradient\n",
    "\n",
    "다른 방법으로 흔한 최적화 기법을 사용할 수도 있습니다.\n",
    "\n",
    "정책 파라미터가 움직이는 방향에 대한 이득의 변화량을 계산하고 이득이 커지도록 파라미터를 조정할 수 있습니다.\n",
    "\n",
    "이런 방식을 PG(policy gradients)라고 부릅니다.\n",
    "\n",
    "(이쯤에서 류성원님이 말하셨던 MCMC가 재조명 됩니다!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to OpenAI Gym\n",
    "\n",
    "강화학습에서 가장 난관중 하나가 바로, 행위자를 학습시킬 환경을 조성하는 것입니다.\n",
    "\n",
    "하지만 현실에서 많은 경우, 반복 학습을 할 환경을 조성하는 것 조차 어렵습니다. (수행 시간까지 고려해서)\n",
    "\n",
    "그래서 시뮬레이션 환경이 필요합니다.\n",
    "\n",
    "바둑으로 예를 들면, 다이소에 가서 미니 바둑판 100개를 사고 마구 둬볼 필요 없이,\n",
    "\n",
    "컴퓨터로 만든 바둑 시뮬레이션 프로그램을 사용해야 합니다.\n",
    "\n",
    "OpenAI gym는 이를 위한 툴킷입니다. (바둑, 보드게임, 2~3차원 물리적 상황들을 위한 시뮬레이션이 마련되어 있습니다.)\n",
    "\n",
    "![Image](figures/4_.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The typical imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Run a demo of the environment\n",
    "observation = env.reset()\n",
    "cum_reward = 0\n",
    "frames = []\n",
    "for t in range(5000):\n",
    "    # Render into buffer. \n",
    "    plt.imshow(env.render(mode = 'rgb_array'))\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "    displa\n",
    "env.render(close=True)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Policies\n",
    "\n",
    "주먹을 불끈 쥐고 신경망으로 정책을 만들어 봅시다. 이세돌을 박살 낼 준비가 되었습니다.\n",
    "\n",
    "행위자의 관찰이 이 신경망의 인풋 노드가 될 것입니다. 마찬가지로 신경망의 아웃풋은 행위가 됩니다.\n",
    "\n",
    "CartPole 환경의 경우, \n",
    "\n",
    "인풋은 *각도*, *각속력*, *속력*, *위치* 라는 4개 차원이므로 인풋 노드는 4개입니다.\n",
    "\n",
    "아웃풋 노드는 한개입니다. 이 한개의 노드가 의미하는 바는 무엇일까요? (깜짝 Quiz 타임!)\n",
    "\n",
    "1. 아웃풋은 왼쪽 또는 오른쪽이라는 바이너리 형태이다.\n",
    "2. 확률 p값이다. (p = P(왼쪽으로갈 확률) = 1 - P(오른쪽으로 갈 확률))\n",
    "\n",
    "![Image](figures/1_.png)\n",
    "\n",
    "![Image](figures/5_.png)\n",
    "\n",
    "정답은 2번입니다.\n",
    "\n",
    "유전알고리즘이 그러하듯, 확정적인 답을 내지 않음으로써 지역최적값에 머무르는 것을 방지합니다.\n",
    "\n",
    "이런 접근방식은 행위자가\n",
    "\n",
    "- 이미 좋다고 알려진 방식을 선택\n",
    "- 새로운 좋은 방식을 탐색\n",
    "\n",
    "두개 선택지 사이에 균형을 갖추도록 합니다.\n",
    "\n",
    "CartPole 환경은 한가지 특징이 있습니다. 환경의 현재 상태 정보만으로 정책을 수행해도 괜찮다는 것입니다. -*Markov Process*\n",
    "\n",
    "CartPole의 입력변수에 속력정보가 없다면 더이상 마코브 과정이 아니게 됩니다. (이전 정보를 통해 속력을 계산해야 하므로)\n",
    "\n",
    "\n",
    "마찬가지로 로봇청소기의 환경도 마코브 과정이 아닐 수 있습니다. (먼지를 한번 닦은 곳을 또 닦으면 의미가 없으니)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "n_inputs = 4\n",
    "    n_hidden = 4\n",
    "    n_outputs = 1\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "    hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu, weights_initializer=initializer)\n",
    "    logits = fully_connected(hidden, n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "    outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "    p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "    action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 처럼 간단하게 신경망을 구축할 수 있습니다.\n",
    "\n",
    "이 신경망을 어떻게 트레이닝 할지 다음에서 설명합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Actions: The Credit Assignment Problem\n",
    "\n",
    "앞의 신경망이 간단하다고 해서 신경망의 트레이닝도 다른 신경망처럼 간단할 것이라 생각했다면 오산입니다.\n",
    "\n",
    "지금 상태에서 우리는 교사학습을 진행할 수 없습니다. 각 단계마다 어떤 선택이 최선인지 우리는 알지 못합니다.\n",
    "\n",
    "강화학습에는 교사학습이 가능한 레이블은 없지만 지침이 되어주는 **보상**과 **처벌**이 있습니다.\n",
    "\n",
    "**보상**과 **처벌**은 일반적으로 즉각적이지 못하고 불규칙적입니다.\n",
    "\n",
    "> CartPole환경을 예로 들어 봅시다.\n",
    "\n",
    "> 시뮬레이션을 통해 100번의 스텝을 거쳐갔다고 칩니다. 이때 이 100번의 스텝이 좋은지 나쁜지 어떻게 알 수 있을까요?\n",
    "\n",
    "> 우리가 아는 정보는 100번의 스텝 이후 막대기가 넘어졌다는 사실 뿐입니다.\n",
    "\n",
    "> 하지만 막대기가 100번째의 행위때문에 넘어졌는지 \n",
    "\n",
    "> 아니면, 96번째의 잘못된 행위로 인해 넘어져 가다가 100번째에 넘어진 것인지는 알지 못합니다.\n",
    "\n",
    "이 문제를 *Credit Assignment Problem*이라고 부릅니다.\n",
    "\n",
    "행위자가 보상을 받더라도, 어떤 행위로 인해 보상을 받는지 알 길이 없는 경우가 많습니다.\n",
    "\n",
    "이 문제를 해결하기 위해 많이 쓰이는 전략중 하나로\n",
    "\n",
    "행위를 **행위가 실행된 후 받은 보상들의 합**으로 평가하는 것입니다. \n",
    "\n",
    "나중에 실행될수록 할인율을 적용합니다.\n",
    "\n",
    "다음의 예시로 설명합니다.\n",
    "\n",
    "![Image](figures/6_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 물론, 나쁜 행위로인해 막대기가 걷잡을 수 없이 빠르게 넘어질 때,\n",
    "\n",
    "> 그 후에 좋은 행위들이 이어지게 된다면, 좋은 행위도 나쁜 점수를 받게 됩니다.\n",
    "\n",
    "> 하지만 시뮬레이션을 충분히 돌린다면 결국 제 점수를 찾아갈 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients\n",
    "\n",
    "PG(policy gradients)에 대해 앞에서 설명했었습니다.\n",
    "\n",
    "> 정책 파라미터가 움직이는 방향에 대한 보상의 변화량을 계산하고 보상이 커지도록 파라미터를 조정하는 것\n",
    "\n",
    "PG에서 인기있는 알고리즘중 하나가 이번에 소개할 REINFORCE Algorithms 입니다.\n",
    "\n",
    "REINFORCE Algorithms을 적용해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1 \n",
    "\n",
    "> 게임을 몇판 플레이해줍니다\n",
    "\n",
    "> 플레이하는동안 각 스텝마다 그레디언트를 만들어 줍니다.\n",
    "\n",
    "> 이때 그레디언트는 **지금의 선택(왼쪽으로 갈 확률 p)**을 더 강화하도록 학습합니다. \n",
    "\n",
    "> 이는 현재 행위가 최선의 행위라고 가정하는 것을 의미합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2\n",
    "\n",
    "> 어느정도 플레이가 되었다면\n",
    "\n",
    "> 각 에피소드에서 각 스텝의 행위에 대한 보상점수를 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3\n",
    "\n",
    "> 이전에 계산했던 그레디언트에 보상점수를 곱합니다.\n",
    "\n",
    "- 계산된 행위의 보상점수가 양수라면,\n",
    "\n",
    "> 이는 이 행위가 잘한 행위임을 뜻하며, 우리가 1에서 설정한 그레디언트가 괜찮다는 뜻입니다.\n",
    "\n",
    "- 계산된 행위의 보상점수가 음수라면,\n",
    "\n",
    "> 이는 이 행위가 잘못된 행위임을 뜻하며, 우리가 1에서 설정한 그레디언트가 잘못되었다는 뜻입니다. (반대라는 뜻)\n",
    "\n",
    "> 음수인 보상점수가 곱해져 그레디언트 방향이 반대가 됩니다. (잘못된 그레디언트가 수정되었습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4\n",
    "\n",
    "> 모든 에피소드로 부터 얻은 그레디언트들을 평균내줍니다.\n",
    "\n",
    "> 이렇게 얻은 그레디언트는 이제 GD를 위해 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-16 06:10:52,588] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    \n",
    "    n_inputs = 4\n",
    "    n_hidden = 4\n",
    "    n_outputs = 1\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "    hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu, weights_initializer=initializer)\n",
    "    logits = fully_connected(hidden, n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "    outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "    p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "    # 왼쪽으로 갈 확률과 오른쪽으로 갈 확률\n",
    "    action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "    # 두개의 선택지 사이에서 확률적 선택을 해줍니다.\n",
    "    y = 1. - tf.to_float(action)    \n",
    "    # y가 0이라면 왼쪽 이동, 1이라면 오른쪽 이동\n",
    "    learning_rate = 0.01\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    # 비용함수\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    # 그레디언트 알고리즘\n",
    "    grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "    # minimize하지 않고 그레디언트만 구합니다.\n",
    "    gradients = [grad for grad, variable in grads_and_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이렇게 만들어준 그래프는 학습하면서 그래디언트를 만들어가게 됩니다.\n",
    "\n",
    "> 몇번의 시뮬레이션 이후 \n",
    "\n",
    "> 우리는 앞서 3, 4번에서 이야기 했던 작업 (만든 그레디언트를 적용하는 일)이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    \n",
    "    gradient_placeholders = []\n",
    "    grads_and_vars_feed = []\n",
    "    for grad, variable in grads_and_vars:\n",
    "        gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "        gradient_placeholders.append(gradient_placeholder)\n",
    "        grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "    \n",
    "    training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "# 스텝마다의 리워드 계산\n",
    "\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]\n",
    "# 종합 리워드 계산과 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working..\n",
      "working..\n",
      "working..\n",
      "working..\n",
      "working..\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    n_iterations = 25\n",
    "    n_max_steps = 100\n",
    "    n_games_per_update = 5\n",
    "    save_iterations = 5\n",
    "    discount_rate = 0.95\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for iteration in range(n_iterations):\n",
    "            all_rewards = []\n",
    "            all_gradients = []\n",
    "            for game in range(n_games_per_update):\n",
    "                current_rewards = []\n",
    "                current_gradients = []\n",
    "                obs = env.reset()\n",
    "                for step in range(n_max_steps):\n",
    "                    action_val, gradients_val = sess.run(\n",
    "                        [action, gradients],\n",
    "                        feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "                    obs, reward, done, info = env.step(action_val[0][0])\n",
    "                    current_rewards.append(reward)\n",
    "                    current_gradients.append(gradients_val)\n",
    "                    if done:\n",
    "                        break\n",
    "                all_rewards.append(current_rewards)\n",
    "                all_gradients.append(current_gradients)\n",
    "            \n",
    "            all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
    "            feed_dict = {}\n",
    "            for var_index, grad_placeholder in enumerate(gradient_placeholders):\n",
    "                mean_gradients = np.mean(\n",
    "                    [reward * all_gradients[game_index][step][var_index]\n",
    "                        for game_index, rewards in enumerate(all_rewards)\n",
    "                        for step, reward in enumerate(rewards)],\n",
    "                    axis=0)\n",
    "                feed_dict[grad_placeholder] = mean_gradients\n",
    "            sess.run(training_op, feed_dict=feed_dict)\n",
    "            if iteration % save_iterations == 0:\n",
    "                print(\"working..\")\n",
    "                saver.save(sess, \"./my_policy_net_pg.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
