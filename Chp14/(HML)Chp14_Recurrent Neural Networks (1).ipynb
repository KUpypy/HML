{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chp14. Recurrent Neural Networks\n",
    "\n",
    "- 발표자 : 송서하\n",
    "- 발표일 : 2017.9.2(토)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 야구에서 타자는 공의 트랙을 예상하면서 즉시 달리기를 시작합니다.\n",
    "\n",
    "```\n",
    "미래를 예언하는 것은 친구의 말 끝을 예측하거나 \n",
    "교수님의 눈치를 보는 것처럼 항상하는 일입니다.\n",
    "이 장에서는 미래를 예측할 수있는 네트워크\n",
    "Recurrent neural networks(RNN)에 대해 논의 합니다.\n",
    "\n",
    "RNN은 주가와 같은 시계열 데이터를 분석하고 언제 사거나 팔지 판단할 수 있습니다.\n",
    "자율 주행 시스템에서 자동차 궤도를 예측하고 사고를 피할 수 있습니다.\n",
    "\n",
    "RNN은 고정된 크기의 입력이 아닌, 임의의 길이와 순서에 대해 분석 가능합니다.\n",
    "ex) 문장, 문서, 오디오 샘플 => 자연어 처리 (NLP)\n",
    "\n",
    "RNN을 응용한 창조작업도 가능합니다. \n",
    "멜로디에서 가장 가능성이 높은 다음 음표들을 예측 하고 \n",
    "그중 중 하나를 선택하여 연주하도록 요청할 수 있습니다. \n",
    "(구글의 마젠타 Magenta 프로젝트)\n",
    "\n",
    "1. 이 장에서는 RNN의 근본적인 개념과\n",
    "2. 직면하는 주요 문제 (사라지는 / 폭발하는 그라디언트)\n",
    "3. 이를 해결하기 위해 널리 사용되는 솔루션 (LSTM 및 GRU)\n",
    "4. 기계 번역 시스템의 아키텍처를 살펴 봅니다.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neurons\n",
    "\n",
    "```\n",
    "지금까지 입력 경로에서 출력 계층으로의 한 방향으로만 흐르는 \n",
    "feedforward 신경망을 주로 살펴 보았습니다.\n",
    "RNN은 feedforward 신경망과 매우 흡사합니다. 단, 역방향 연결이 있습니다. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" width=100 align=right>\n",
    "\n",
    "입력을 수신하고, 출력을 생성하며, 그 출력을 입력하는\n",
    "\n",
    "하나의 뉴런 구성된 가장 간단한 가능한 RNN을 살펴 보겠습니다. \n",
    "\n",
    "각 시간 단계 t에서 RNN은 \n",
    "\n",
    "이전 시간(t-1)의 자체 출력뿐만 \n",
    "\n",
    "아니라 현재(t)의 입력도 받습니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2.png\" width=300 align=right>\n",
    "\n",
    "시간 축에 대해 네트워크를 나타낼 수 있습니다. \n",
    "\n",
    "네트워크는 한개이지만 시간축에 따라 \n",
    "\n",
    "여러개가 되는 것처럼 보입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"3.png\" width=350 align=right>\n",
    "\n",
    "\n",
    "RNN의 레이어는 쉽게 만들 수 있습니다. \n",
    "\n",
    "각 시간 단계 t에서, 모든 뉴런은 \n",
    "\n",
    "현재 입력 x(t)와 \n",
    "\n",
    "이전 출력 y(t-1)을 모두받습니다.\n",
    "\n",
    "(레이어에서 입출력은 모두 벡터, 단일 뉴런에서는 스칼라)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"4.png\" width=280 align=left>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "인스턴스 1개로부터 입력을 받는 단일 RNN의 출력을 계산할 수 있습니다.\n",
    "\n",
    "각각의 뉴런들이 받는 weight는 2개 한세트입니다. 하나는 현재의 입력(t)에 대한 것이고 \n",
    "\n",
    "다른 하나는 이전 시간 단계의 출력(t-1)입니다.\n",
    "\n",
    "이 weight 벡터를 $W_x$와 $W_y$라고 해둡니다. ($b$는 바이어스 항이고 $\\phi$( · )는 활성화 함수입니다.)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"5.png\" width=380 align=left>\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "feedforward 신경망과 마찬가지로,\n",
    "\n",
    "(한개의 mini-batch 안에서) 한번에 모든 인스턴스에 대한 행렬 계산이 가능합니다.\n",
    "\n",
    "- $Y$    \n",
    "\n",
    "> [m by n1], m은 인스턴스의 수, n1은 현재 레이어 뉴런의 수입니다.\n",
    "\n",
    "- $W_y$\n",
    "   \n",
    "> [n1 by n1]\n",
    "\n",
    "- $X$ \n",
    "\n",
    "> [m by n2], n2은 인풋의 차원입니다.\n",
    "\n",
    "- $W_y$\n",
    "  \n",
    "> [n2 by n1]\n",
    "\n",
    "- $b$\n",
    "\n",
    "> [n1 by 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y_t$는 $X_t$와 $Y_{t-1}$로 이루어 지는 함수이며, $Y_{t-1}$또한 $X_{t-1}$과 $Y_{t-2}$로 이루어 지는 함수입니다.\n",
    "\n",
    "이것은 시간 t = 0 이후의 모든 X(즉, $X_0, X_1, ..., X_t$)가\n",
    "\n",
    "$Y_t$의 입력 함수가 되는 것이라고 생각할 수 있습니다.\n",
    "\n",
    "또한 일반적으로, 첫 번째 단계 t = 0에서는 이전에 출력이 없으므로 모두 0으로 가정합니다.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6.png\" width=350 align=right>\n",
    "\n",
    "지금까지 설명한 기본 셀의 경우 \n",
    "\n",
    "$Y_t, h_t$가 동일하지만 항상 그렇지는 않습니다.\n",
    "\n",
    "시간 t에서 RNN의 출력은 \n",
    "\n",
    "이전 시간 단계의 모든 입력 값의 함수이므로 \n",
    "\n",
    "메모리의 형태를 가질 수 있습니다. \n",
    "\n",
    "시간 단계에 따라 일부 상태를 보존하는 신경망의 일부를 '셀' 이라고합니다. \n",
    "\n",
    "하나의 RNN 또는 RNN의 레이어는 매우 기본적인 세포이지만, 이 장의 뒷부분에서 \n",
    "\n",
    "좀 더 복잡하고 강력한 유형의 세포를 살펴볼 것입니다.\n",
    "\n",
    "t시간의 셀은 $h_{t}$(\"h\"는 \" hidden\"을 나타냄)로 표시하며, 다음과 같은 함수입니다.\n",
    "\n",
    "## $h_{t} = f(h_{t-1}, x_{t})$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"7.png\" width=400 align=right>\n",
    "\n",
    "2사분면은 시퀀스-시퀀스 입니다.\n",
    "\n",
    "RNN은 일련의 입력을 동시에 받아\n",
    "\n",
    "일련의 출력을 생성 할 수 있습니다.\n",
    "\n",
    "이런 네트워크는 주식 가격과 같은 \n",
    "\n",
    "시계열데이터를 분석하기 용이합니다. \n",
    "\n",
    "최근 N 일간의 가격을 피드로 제공하고 \n",
    "\n",
    "미래에 전환 될 가격을 출력해야합니다.\n",
    "\n",
    "<br>\n",
    "1사분면처럼 일련의 입력을 제공하고 마지막 입력을 제외한 모든 출력을 무시할 수도 있습니다.\n",
    "\n",
    "즉, 이것은 sequence-to-vector 네트워크입니다. \n",
    "\n",
    "영화 리뷰로 예를 들자면, 일련의 단어를 네트워크에 입력하고 네트워크는 정서 점수를 출력합니다.\n",
    "\n",
    "(ex: -1 [증오], +1 [사랑])\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"7.png\" width=400 align=right>\n",
    "\n",
    "\n",
    "<br>\n",
    "3사분면은 \n",
    "\n",
    "벡터 대 시퀀스 네트워크입니다. \n",
    "\n",
    "이미지 분석으로 예를 들면, \n",
    "\n",
    "입력은 이미지이고 \n",
    "\n",
    "출력은 이미지의 캡션이 될 수 있습니다.\n",
    "\n",
    "<br><br><br><br>\n",
    "마지막으로, 4사분면은 인코더(시퀀스 - 벡터 네트워크)와 디코더(벡터 - 시퀀스 네트워크)입니다.\n",
    "\n",
    "예를 들어, 한 언어에서 다른 언어로 문장을 번역하는 데 사용할 수 있습니다.\n",
    "\n",
    "네트워크에 하나의 언어로 문장을 보내면 인코더는 이 문장을 단일 벡터 표현으로 변환 한 다음 \n",
    "\n",
    "디코더가 이 벡터를 다른 언어로 문장으로 디코딩합니다. \n",
    "\n",
    "Encoder-Decoder 라고 하는 이 2단계 모델은 문장의 마지막 단어가 번역의 첫 단어에 \n",
    "\n",
    "영향을 미치므로 번역하기 전에 전체 문장을들을 때까지 기다려야합니다.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RNNs in TensorFlow\n",
    "\n",
    "<img src=\"3.png\">\n",
    "\n",
    "```\n",
    "매우 단순한 RNN 모델을 구현하여 이해합니다. \n",
    "하이퍼탄젠트 activation을 사용해 위처럼 5개의 뉴런으로 구성된 RNN을 구축합니다.\n",
    "RNN은 두단계의 시간동안 3개의 인풋 백터를 받는다고 설정합니다. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99945915  0.62154311  0.98704058 -0.999946    0.98809916]\n",
      " [-1.         -0.99214089  0.99999791 -0.99999994  0.99999875]\n",
      " [-1.         -0.99999267  1.         -1.          1.        ]\n",
      " [-1.         -0.99999958  0.98988044  1.          0.99893618]]\n",
      "[[-1.         -1.          1.         -0.99881339  1.        ]\n",
      " [-0.99831319  0.33889857 -0.88244802 -0.98821503  0.51282984]\n",
      " [-1.         -0.99999917  0.999879   -0.99954444  0.99999875]\n",
      " [-0.99999994 -0.995601   -0.99718505 -0.73239839 -0.80686736]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs, n_neurons = 3, 5\n",
    "# 인풋백터는 3차원, 레이어의 뉴런은 5개입니다.\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])\n",
    "# 각 미니배치는 인트턴스가 4개씩 있다고 설정하였습니다.\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "# 모델을 실행하려면 각 단계에서 인풋을 지정해야 하는 코드입니다.\n",
    "\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons], dtype=tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
    "# 이 네트워크는 레이어 두개의 피드 포워드 신경망과 비슷합니다.\n",
    "# 동일한 가중치와 바이어스 조건이 두 계층에서 공유되며\n",
    "# 각 계층에서 입력을 공급하고 각 계층의 출력을 얻습니다\n",
    "\n",
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})\n",
    "    \n",
    "print(Y0_val, Y1_val, sep=\"\\n\")\n",
    "# 각 시간 단계에서의 출력값을 확인합니다.\n",
    "# 참 쉽죠!!?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Unrolling Through Time\n",
    "\n",
    "```\n",
    "이전의 코드에서 그래프를 그리는 부분만 바꿨습니다.\n",
    "이전의 코드와 똑같이 동작합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.74405742 -0.39245805  0.29843029  0.5614053  -0.75799692]\n",
      " [-0.99934834 -0.47331062  0.09823982  0.95483166 -0.40133765]\n",
      " [-0.99999851 -0.5469045  -0.11023035  0.99620581  0.14007743]\n",
      " [-0.99168289  0.7131893  -0.77603054 -0.84732074  0.99992937]]\n",
      "[[-0.99999934 -0.20906153 -0.88850057  0.99791402  0.95167339]\n",
      " [ 0.6627425  -0.28957993 -0.37195262  0.49736682 -0.67464811]\n",
      " [-0.99969625  0.11432403 -0.70250231  0.98536325  0.933088  ]\n",
      " [-0.93764913  0.8005501   0.48100898  0.43819082  0.98307931]]\n",
      "[[-0.99999934 -0.20906153 -0.88850057  0.99791402  0.95167339]\n",
      " [ 0.6627425  -0.28957993 -0.37195262  0.49736682 -0.67464811]\n",
      " [-0.99969625  0.11432403 -0.70250231  0.98536325  0.933088  ]\n",
      " [-0.93764913  0.8005501   0.48100898  0.43819082  0.98307931]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs, n_neurons = 3, 5\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "# 이전처럼 인풋을 위한 플레이스홀더를 사용합니다.\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "# BasicRNNCell을 생성합니다.\n",
    "# 각 시간 단계마다 하나씩 셀을 생성합니다.\n",
    "\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(\n",
    "                          basic_cell, [X0, X1], dtype=tf.float32)\n",
    "# static_rnn()이라는 함수는 입력텐서를 받아 셀을 엮어 RNN을 만들어 줍니다.\n",
    "# 이 함수는 앞서 말했던 0으로 채워진 초기값도 만들어줍니다.\n",
    "# static_rnn() 함수는 두 개의 객체를 반환합니다. \n",
    "# 첫 번째는 Python-list로 된 각 시간 단계에 대한 출력 텐서입니다.\n",
    "# 두 번째는 네트워크의 셀을 포함하는 텐서입니다. \n",
    "# 기본 셀을 사용하는 경우 최종 상태는 단순히 마지막 출력과 같습니다.\n",
    "\n",
    "Y0, Y1 = output_seqs\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    init.run()\n",
    "    # 꼭 이렇게 두번 feed해줘야 하나요?\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})\n",
    "    states_val = sess.run(states, feed_dict={X0: X0_batch, X1: X1_batch})\n",
    "    \n",
    "print(Y0_val, Y1_val, states_val, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "50단계의 시간으로 구성한다면 \n",
    "50 개의 입력 플레이스홀더 및 50 개의 출력 텐서를 정의하는 것은 \n",
    "비생산적입니다. \n",
    "\n",
    "또한 세션 실행시 50 개의 각 플레이스홀더에 피드를 제공하고 \n",
    "50 개의 출력을 조작해야합니다. \n",
    "\n",
    "이 작업을 단순화합시다! \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.61080521 -0.69326246 -0.25279245  0.47383082 -0.61794764]\n",
      "  [-0.99982709 -0.99999559 -0.99985635  0.9999994  -0.99999881]]\n",
      "\n",
      " [[-0.9866333  -0.99569213 -0.9587661   0.99584126 -0.99786222]\n",
      "  [ 0.25527331 -0.88628924 -0.13369818  0.14054763  0.35202315]]\n",
      "\n",
      " [[-0.99962521 -0.99994862 -0.99851501  0.99997574 -0.99999017]\n",
      "  [-0.98901439 -0.99990916 -0.99691141  0.99985337 -0.99964011]]\n",
      "\n",
      " [[-0.97431576 -0.9508335  -0.95992136  0.99997121 -0.99996179]\n",
      "  [-0.66699362 -0.99169588 -0.91625541  0.97433281 -0.92124861]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs, n_neurons, n_steps = 3, 5, 2\n",
    "# n_step이라는 변수를 추가했습니다.\n",
    "X_batch = np.array([\n",
    "    [[0, 1, 2], [9, 8, 7]],\n",
    "    [[3, 4, 5], [0, 0, 0]],\n",
    "    [[6, 7, 8], [6, 5, 4]], \n",
    "    [[9, 0, 1], [3, 2, 1]],\n",
    "])\n",
    "# 배치를 처음부터 시간축을 포함한 3차원 배열로 구성합니다.\n",
    "# 첫번째 차원은 미니배치의 크기(인스턴스의 수)이며,\n",
    "# 두번째 차원은 시간 단계\n",
    "# 세번째 차원은 인풋 백터의 차원입니다.\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "X_seqs = tf.unstack(tf.transpose(X, perm=[1, 0, 2]))\n",
    "# transpose () 함수를 사용해 처음 두 차원을 교환합니다.\n",
    "# 결론적으로 시간 단계가 이제 첫 번째 차원이되도록합니다.\n",
    "# unstack () 함수를 사용하여 첫 번째 차원(시간)을 따라 python-list를 추출합니다.\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(\n",
    "                          basic_cell, X_seqs, dtype=tf.float32)\n",
    "outputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2])\n",
    "# stack() 함수를 사용하여 모든 출력 텐서를 하나의 텐서로 병합합니다.\n",
    "# 다시 첫 번째 두 차원을 교환합니다.\n",
    "# 이로써 최종 출력 형태로 [None, n_steps, n_neurons]을 얻습니다.\n",
    "# 다시 첫 번째 차원은 배치의 크기가 됩니다.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    init.run()\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})\n",
    "    \n",
    "print(outputs_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Unrolling Through Time\n",
    "\n",
    "```\n",
    "Static한 방법은 시간마다 하나의 셀을 포함하는 그래프를 작성합니다. \n",
    "50 개의 타임 스텝이 있다면, 그리고 그걸 텐서보드에 그린다면 흉물스러울 것입니다.\n",
    "정방향 패스 중에 모든 텐서 값을 저장해야하므로 역 전파 중에\n",
    "메모리 부족(OOM) 오류가 발생할 수 있습니다. \n",
    "(GPU의 메모리가 제한되어 있으니까요!)\n",
    "\n",
    "다행히도 텐서플로우에는 dynamic_rnn () 함수가 있습니다.\n",
    "dynamic_rnn() 함수는 while_loop() 함수를 사용하여 셀을 적절한 횟수만큼 실행하고 \n",
    "OOM 오류를 피하기 위해 역 전파 중 GPU의 메모리를 \n",
    "CPU 메모리로 스왑하려는 경우 swap_memory = True로 설정할 수 있습니다.\n",
    "\n",
    "편리하게도, 모든 시간 단계 (shape [None, n_steps, n_inputs])에서 \n",
    "모든 입력에 대해 단일 텐서를 받아들이고 \n",
    "모든 시간 단계 (shape [None, n_steps, n_neurons])에서 \n",
    "모든 출력에 대해 단일 텐서를 출력합니다\n",
    "\n",
    "한마디로 stack, unstack 또는 transpose 할 필요가 없습니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.66766584 -0.16598244  0.35968059  0.89901453 -0.57601589]\n",
      "  [ 0.97810423  0.99936968 -0.2470468   0.99999231 -0.99999976]]\n",
      "\n",
      " [[ 0.94730133  0.79665494  0.4533332   0.99954754 -0.99841619]\n",
      "  [-0.43821493 -0.27105731  0.04647241 -0.31969595  0.02313256]]\n",
      "\n",
      " [[ 0.99267673  0.98184109  0.53794301  0.99999803 -0.99999535]\n",
      "  [ 0.52357638  0.98091531 -0.06146135  0.99879545 -0.9999305 ]]\n",
      "\n",
      " [[-0.98241347  0.98621559 -0.97608751  0.68959761 -0.99178451]\n",
      "  [ 0.14115636  0.54938579 -0.15978913  0.92743194 -0.93500626]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs, n_neurons, n_steps = 3, 5, 2\n",
    "X_batch = np.array([\n",
    "    [[0, 1, 2], [9, 8, 7]],\n",
    "    [[3, 4, 5], [0, 0, 0]],\n",
    "    [[6, 7, 8], [6, 5, 4]], \n",
    "    [[9, 0, 1], [3, 2, 1]],\n",
    "])\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    init.run()\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})\n",
    "    \n",
    "print(outputs_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Variable Length Input Sequences\n",
    "\n",
    "```\n",
    "지금까지 입력 시퀀스가 두 단계로 고정된 경우만 사용했습니다. \n",
    "입력 시퀀스의 길이가 가변적 인 경우 \n",
    "dynamic_rnn() (또는 static_rnn()) 함수를 호출 할 때 \n",
    "sequence_length 매개 변수를 설정해야합니다. \n",
    "각 인스턴스에 대한 입력 시퀀스의 길이를 나타내는 1차원 텐서 여야합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.7537089  -0.8848117  -0.26607832 -0.39816904  0.57358909]\n",
      "  [ 0.99230874 -0.97077483  0.99287438 -0.99993104 -0.99979764]]\n",
      "\n",
      " [[-0.37313765 -0.98680699  0.6439541  -0.9688412  -0.48761949]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.19483687 -0.9985581   0.9470579  -0.99883682 -0.93770874]\n",
      "  [ 0.96220136 -0.96806157  0.99312234 -0.96813357 -0.99526197]]\n",
      "\n",
      " [[ 0.99977452  0.99990374  0.9999814  -0.26485538 -0.99996173]\n",
      "  [ 0.76398182 -0.20260505  0.98409259 -0.16830882 -0.9046945 ]]]\n",
      "\n",
      "[[ 0.99230874 -0.97077483  0.99287438 -0.99993104 -0.99979764]\n",
      " [-0.37313765 -0.98680699  0.6439541  -0.9688412  -0.48761949]\n",
      " [ 0.96220136 -0.96806157  0.99312234 -0.96813357 -0.99526197]\n",
      " [ 0.76398182 -0.20260505  0.98409259 -0.16830882 -0.9046945 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs, n_neurons, n_steps = 3, 5, 2\n",
    "X_batch = np.array([\n",
    "    [[0, 1, 2], [9, 8, 7]],\n",
    "    [[3, 4, 5], [0, 0, 0]], # 1개짜리 시퀀스는 제로패딩을 줬습니다.\n",
    "    [[6, 7, 8], [6, 5, 4]], \n",
    "    [[9, 0, 1], [3, 2, 1]],\n",
    "])\n",
    "seq_length_batch = np.array([2, 1, 2, 2])\n",
    "# 가변적인 시퀀스 길이 정보입니다.\n",
    "\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(\n",
    "    basic_cell, X, dtype=tf.float32, sequence_length=seq_length)\n",
    "# dynamic_rnn함수에 시퀀스 길이를 인풋으로 던져줬습니다.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run(\n",
    "        [outputs, states],\n",
    "        feed_dict={X: X_batch, seq_length: seq_length_batch})\n",
    "    \n",
    "print(outputs_val, states_val, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Variable-Length Output Sequences\n",
    "\n",
    "```\n",
    "출력 시퀀스에도 가변 길이가 있으면 어떻게 될까요? \n",
    "각 시퀀스의 길이를 미리 알고있는 경우 위에서 설명한대로 \n",
    "sequence_length 매개 변수를 설정할 수 있습니다. \n",
    "\n",
    "유감스럽게도, 일반적으로 이것은 가능하지 않습니다.\n",
    "예를 들어, 번역 된 문장의 길이가 어찌 될지는 번역 전에 알 수가 없습니다.\n",
    "\n",
    "이 경우, 가장 일반적인 솔루션은 EOS 토큰(end-of-sequence token)\n",
    "이라고하는 특별한 출력을 정의하는 것입니다(이 장의 뒷부분에서 설명 할 것입니다).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "## 240page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 7.03447\n",
      "Epoch 10 MSE = 4.29253\n",
      "Epoch 20 MSE = 3.19508\n",
      "Epoch 30 MSE = 2.80738\n",
      "Epoch 40 MSE = 2.74301\n",
      "Epoch 50 MSE = 1.37748\n",
      "Epoch 60 MSE = 1.58622\n",
      "Epoch 70 MSE = 1.0794\n",
      "Epoch 80 MSE = 1.29272\n",
      "Epoch 90 MSE = 1.03554\n",
      "[[ 1.81491685]\n",
      " [ 1.92349875]\n",
      " [ 1.28053606]\n",
      " [ 1.68693447]\n",
      " [ 1.65671778]\n",
      " [ 2.215832  ]\n",
      " [ 2.13881755]\n",
      " [ 1.34910572]\n",
      " [-2.6932559 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "data = np.c_[np.ones([m, 1]), housing.data, housing.target]\n",
    "\n",
    "n_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 200    \n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "\n",
    "def data_shuffle(epoch, data):\n",
    "    \n",
    "    data = data[:]\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def fetch_batch(shuffled, batch_index, batch_size):\n",
    "    \n",
    "    start_index = batch_index * batch_size\n",
    "    end_index = start_index + batch_size\n",
    "    data = shuffled[start_index:end_index]\n",
    "    X_batch, y_batch = data[:,:-1], data[:,-1].reshape(-1, 1)\n",
    "    return X_batch, y_batch\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with  tf.Graph().as_default():\n",
    "\n",
    "    raw_X = tf.placeholder(tf.float32, shape=(None, n+1))\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    theta = tf.Variable(tf.random_uniform([n+1, 1], -1, 1))\n",
    "\n",
    "    X = tf.nn.l2_normalize(raw_X, 0)\n",
    "    y_pred = tf.matmul(X, theta)\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(mse) \n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(n_epochs):\n",
    "            shuffled = data_shuffle(epoch, data)\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = fetch_batch(shuffled, batch_index, batch_size)\n",
    "    # 꼭 이렇게 두번 feed해줘야 하나요?\n",
    "                \n",
    "                current_mse = sess.run(mse, feed_dict={raw_X: X_batch, y: y_batch})\n",
    "                sess.run(training_op, feed_dict={raw_X: X_batch, y: y_batch})\n",
    "                \n",
    "                if epoch % 10 == 0 and batch_index == n_batches - 1:\n",
    "                    print(\"Epoch\", epoch, \"MSE =\", current_mse)\n",
    "        best_theta = theta.eval()\n",
    "        \n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
